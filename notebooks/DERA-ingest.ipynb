{
 "cells": [
  {
   "cell_type": "raw",
   "id": "629351e6-27cb-463f-aacd-66c0fa3571de",
   "metadata": {},
   "source": [
    "# 001: Share, Price, Revenue -> 314 Market Cap\n",
    "# 002: Revenue, Debt, Float -> 10101.99 Market Cap\n",
    "# 003: Revenue, Price, Share -> 271.8 Market Cap\n",
    "# 004: Revenue, Float, Float2 -> 3141.59 Market Cap\n",
    "\n",
    "df = pd.DataFrame({'adsh' : ['001', '002', '001', '002',\n",
    "                          '001', '002', '003', '003', '003', '004', '004', '004', '005'],\n",
    "                   'sic' : ['4911', '4911', '4911', '4911',\n",
    "                          '4911', '4911', '4911', '4911', '4911', '3210', '3210', '3210', '666'],\n",
    "                   'tag' : ['Shares', 'Revenue', 'Price', 'Debt', 'Revenue', 'Float', 'Revenue', 'Price', 'Shares', 'Revenue', 'Float', 'Float2','Nothing'],\n",
    "                   'ddate' : ['01-01-01', '02-02-02', '01-01-01', '02-02-02', '01-01-01', '02-02-02', '03-03-03', '03-03-03', '03-03-03', '04-04-04', '04-04-04', '04-04-04','05-05-05'],\n",
    "                   'value' : [100, 11.0, 3.14, 2.2, 0.02, 10101.99, 189, 100, 2.718, 1024.48, 2718.28, 3141.59,-1.0]})\n",
    "display(df[df['tag'].isin(['Float'])])\n",
    "\n",
    "# adsh_001[adsh_001['tag']=='Shares']['value'].reset_index() * adsh_001[adsh_001['tag']=='Price']['value'].reset_index()\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        print(key)\n",
    "        df = grouped_df.get_group(key)\n",
    "        df_max = df[df.tag.isin(['Float','Float2'])]\n",
    "        if df_max.empty:\n",
    "            df_max = df[df.tag=='Shares']\n",
    "            if not df_max.empty and not df[df.tag=='Price'].empty:\n",
    "                df_max = df_max.copy()\n",
    "                df_max['value'] = df[df.tag=='Price']['value'].mean() * df_max['value'].squeeze()\n",
    "                df_max['tag'] = 'InferredFloat'\n",
    "        else:\n",
    "            df_max = df_max.sort_values('value', ascending=False).iloc[0].copy()\n",
    "            df_max['tag'] = 'SelectedFloat'\n",
    "        new_df = new_df.append(df_max)\n",
    "    return new_df\n",
    "\n",
    "x = df.groupby(['adsh', 'ddate']).pipe(infer_float)\n",
    "df = df.append(x)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf0869-f139-411a-9b34-51945c12cade",
   "metadata": {},
   "source": [
    "# Ingest SEC DERA data into Trino pipeline\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf87f-0bd3-41e1-acb8-c2508110c510",
   "metadata": {},
   "source": [
    "Run these in a notebook cell if you need to install onto your nb env\n",
    "\n",
    "```python\n",
    "# 'capture' magic prevents long outputs from spamming your notebook\n",
    "%%capture pipoutput\n",
    "\n",
    "# For loading predefined environment variables from files\n",
    "# Typically used to load sensitive access credentials\n",
    "%pip install python-dotenv\n",
    "\n",
    "# Standard python package for interacting with S3 buckets\n",
    "%pip install boto3\n",
    "\n",
    "# Interacting with Trino and using Trino with sqlalchemy\n",
    "%pip install trino sqlalchemy sqlalchemy-trino\n",
    "\n",
    "# Pandas and parquet file i/o\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# OS-Climate utilities to make data ingest easier\n",
    "%pip install osc-ingest-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789567ff-268c-45e8-8f6f-e768ba62f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "import osc_ingest_trino as osc\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7692120-2f4f-48be-9847-0f78a3359bc1",
   "metadata": {},
   "source": [
    "Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ba14b4-0746-4ce6-9c0f-ff680221bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07a1c2b-a2cf-4afe-bbef-041542ac4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "env_var_prefix = 'TRINO'\n",
    "\n",
    "sqlstring = 'trino://{user}@{host}:{port}/'.format(\n",
    "    user = os.environ[f'{env_var_prefix}_USER'],\n",
    "    host = os.environ[f'{env_var_prefix}_HOST'],\n",
    "    port = os.environ[f'{env_var_prefix}_PORT']\n",
    ")\n",
    "sqlargs = {\n",
    "    'auth': trino.auth.JWTAuthentication(os.environ[f'{env_var_prefix}_PASSWD']),\n",
    "    'http_scheme': 'https',\n",
    "    'catalog': 'osc_datacommons_dev'\n",
    "}\n",
    "engine = create_engine(sqlstring, connect_args = sqlargs)\n",
    "connection = engine.connect()\n",
    "\n",
    "trino_bucket = osc.attach_s3_bucket(\"S3_DEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84fa84b-d4c0-4dd6-99da-2cab1641ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "source_bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efe080-b4eb-4713-8fd0-7f1d2e1fd966",
   "metadata": {},
   "source": [
    "Open a Trino connection using JWT for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a0dce7-1762-4605-8617-f2bf6d649f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_catalog = 'osc_datacommons_dev'\n",
    "ingest_schema = 'sandbox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb96687d-74a7-46d8-9944-b63fddf11bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('default',)\n",
      "('information_schema',)\n",
      "('sandbox',)\n"
     ]
    }
   ],
   "source": [
    "# Show available schemas to ensure trino connection is set correctly\n",
    "schema_read = engine.execute(f'show schemas in {ingest_catalog}')\n",
    "for row in schema_read.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d59fd-f47a-4244-9b4a-3cd6d2d1f164",
   "metadata": {},
   "source": [
    "Enter the Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61c5cf5-b15e-4ce0-ad90-e14661950f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b412-e27d-4ccf-a65f-0f00b47affe4",
   "metadata": {},
   "source": [
    "Drop previous tables and schema to start with a fresh slate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b3ea3a7-af40-40f4-8b5a-dea353758b3c",
   "metadata": {},
   "source": [
    "sql = f\"show tables in {ingest_schema}\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd58c7d-a663-4a0a-8838-f1e56aefade9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for view in [ 'assets_by_lei', 'assets_usd_by_lei', 'assets_xyz_by_lei',\n",
    "              'cash_by_adsh_ddate', 'cash_by_lei', 'cash_usd_by_lei', 'cash_xyz_by_lei',\n",
    "              'debt_by_adsh_ddate', 'debt_by_lei', 'debt_usd_by_lei', 'debt_xyz_by_lei',\n",
    "              'financials_by_lei',\n",
    "              'float_by_lei', 'float_usd_by_lei', 'float_xyz_by_lei',\n",
    "              'fy_revenue_by_lei', 'fy_revenue_usd_by_lei', 'fy_revenue_xyz_by_lei',\n",
    "              'fy_income_by_lei', 'fy_income_usd_by_lei', 'fy_income_xyz_by_lei',\n",
    "            ]:\n",
    "    sql = f\"\"\"\n",
    "drop view if exists {ingest_catalog}.{ingest_schema}.{view}\n",
    "\"\"\"\n",
    "    # print(sql)\n",
    "    qres = engine.execute(sql)\n",
    "    # print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b20942-417b-40c7-a6b0-3bda008b1d98",
   "metadata": {},
   "source": [
    "Load `ticker` file (updated sporadically from https://www.sec.gov/include/ticker.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94deb145-7dd9-443c-a93d-38b0e82153ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tname</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aapl</td>\n",
       "      <td>320193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>msft</td>\n",
       "      <td>789019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goog</td>\n",
       "      <td>1652044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amzn</td>\n",
       "      <td>1018724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tsla</td>\n",
       "      <td>1318605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12416</th>\n",
       "      <td>afacu</td>\n",
       "      <td>1849489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12417</th>\n",
       "      <td>afaqw</td>\n",
       "      <td>1841661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12418</th>\n",
       "      <td>aesc</td>\n",
       "      <td>874761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12419</th>\n",
       "      <td>aesew</td>\n",
       "      <td>1708341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12420</th>\n",
       "      <td>aeva-wt</td>\n",
       "      <td>1789029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12421 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tname      cik\n",
       "0         aapl   320193\n",
       "1         msft   789019\n",
       "2         goog  1652044\n",
       "3         amzn  1018724\n",
       "4         tsla  1318605\n",
       "...        ...      ...\n",
       "12416    afacu  1849489\n",
       "12417    afaqw  1841661\n",
       "12418     aesc   874761\n",
       "12419    aesew  1708341\n",
       "12420  aeva-wt  1789029\n",
       "\n",
       "[12421 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'SEC-DERA/ticker.txt')\n",
    "ticker_file.download_file(f'/tmp/dera-ticker.txt')\n",
    "ticker_df = pd.read_csv(f'/tmp/dera-ticker.txt', names=['tname', 'cik'], header=None, sep='\\t', dtype={'tname':'string','cik':'int64'}, engine='c')\n",
    "ticker_dict = dict(zip(ticker_df.cik, ticker_df.tname))\n",
    "\n",
    "ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f886392-62d5-4f94-abc7-82a5f0d9e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "create table if not exists osc_datacommons_dev.sandbox.ticker(\n",
      "    tname varchar,\n",
      "    cik bigint\n",
      ") with (\n",
      "partitioning = array['bucket(tname,20)', 'bucket(cik,20)'],\n",
      "format = 'ORC'\n",
      ")\n",
      "\n",
      "[(True,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/pandas/io/sql.py:1787: SAWarning: index key 'record_count' was not located in columns for table 'ticker'\n",
      "  self.meta.reflect(bind=self.connectable, only=[table_name], schema=schema)\n",
      "/opt/app-root/lib64/python3.8/site-packages/pandas/io/sql.py:1787: SAWarning: index key 'file_count' was not located in columns for table 'ticker'\n",
      "  self.meta.reflect(bind=self.connectable, only=[table_name], schema=schema)\n",
      "/opt/app-root/lib64/python3.8/site-packages/pandas/io/sql.py:1787: SAWarning: index key 'total_size' was not located in columns for table 'ticker'\n",
      "  self.meta.reflect(bind=self.connectable, only=[table_name], schema=schema)\n",
      "/opt/app-root/lib64/python3.8/site-packages/pandas/io/sql.py:1787: SAWarning: index key 'data' was not located in columns for table 'ticker'\n",
      "  self.meta.reflect(bind=self.connectable, only=[table_name], schema=schema)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting 12000 records\n",
      "  ('aapl', 320193)\n",
      "  ('msft', 789019)\n",
      "  ('goog', 1652044)\n",
      "  ...\n",
      "  ('eqha-wt', 1826729)\n",
      "constructed fully qualified table name as: \"sandbox.ticker\"\n",
      "batch insert result: [(12000,)]\n",
      "inserting 421 records\n",
      "  ('eqh-pa', 1333986)\n",
      "  ('eqh-pc', 1333986)\n",
      "  ('ephyu', 1827248)\n",
      "  ...\n",
      "  ('aeva-wt', 1789029)\n",
      "constructed fully qualified table name as: \"sandbox.ticker\"\n",
      "batch insert result: [(421,)]\n"
     ]
    }
   ],
   "source": [
    "ingest_table = 'ticker'\n",
    "columnschema = osc.create_table_schema_pairs(ticker_df)\n",
    "\n",
    "tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{ingest_table}(\n",
    "{columnschema}\n",
    ") with (\n",
    "partitioning = array['bucket(tname,20)', 'bucket(cik,20)'],\n",
    "format = 'ORC'\n",
    ")\n",
    "\"\"\"\n",
    "print(tabledef)\n",
    "qres = engine.execute(tabledef)\n",
    "print(qres.fetchall())\n",
    "ticker_df.to_sql(ingest_table,\n",
    "                 con=engine, schema=ingest_schema, if_exists='replace',\n",
    "                 index=False,\n",
    "                 method=osc.TrinoBatchInsert(batch_size = 12000, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f3bf-da3f-4d85-94ea-0783bebf9ed2",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c28874f-b565-4231-927f-f0db6b9364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleif_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'mtiemann-GLEIF/DERA-matches.csv')\n",
    "gleif_file.download_file(f'/tmp/dera-gleif.csv')\n",
    "gleif_df = pd.read_csv(f'/tmp/dera-gleif.csv', header=0, sep=',', dtype=str, engine='c')\n",
    "gleif_dict = dict(zip(gleif_df.name, gleif_df.LEI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902df315-a8f2-43e3-9d1c-f1bdddeae1a9",
   "metadata": {},
   "source": [
    "Load the SUB, NUM, and TAG tables into Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "467d6d11-b078-46ac-9a5b-4c81eca9878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import uuid\n",
    "\n",
    "# Borrowed/stole this definition from SEC Corp Financials notebook...\n",
    "float_tags = [\n",
    "    'EntityPublicFloat',\n",
    "    'StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest',\n",
    "    'FreeFloat',\n",
    "    'PublicFloat',\n",
    "    'PublicFloatValue',\n",
    "]\n",
    "\n",
    "# These are in priority preference order\n",
    "share_tags = [\n",
    "    'EntityCommonStockSharesOutstanding',\n",
    "    'CommonStockSharesOutstanding',\n",
    "    'SharesOutstanding',\n",
    "    'WeightedAverageNumberOfDilutedSharesOutstanding',\n",
    "    'WeightedAverageNumberOfSharesOutstandingBasic',\n",
    "]\n",
    "\n",
    "# We don't want anything that reports par value, such as CommonStockValue\n",
    "shareprice_tags = [\n",
    "    'SharePrice',\n",
    "    'PerSharePrice',\n",
    "    'MarketValuePerShare',\n",
    "    'SaleOfStockPricePerShare',\n",
    "    'CashPricePerOrdinaryShare',\n",
    "    'TreasurySharesValuePerShare',\n",
    "    'SharesOutstandingPricePerShare',\n",
    "]\n",
    "\n",
    "treasury_share_tags = [\n",
    "    'TreasuryStockShares',\n",
    "    'TreasuryStockShares1',\n",
    "]\n",
    "\n",
    "treasury_value_tags = [\n",
    "    'TreasurySharesMarketValue',\n",
    "    'FairValueOfTreasuryShares',\n",
    "    'MarketValueOfTreasuryShares',\n",
    "]\n",
    "\n",
    "all_float_helper_tags = share_tags + shareprice_tags + treasury_share_tags + treasury_value_tags\n",
    "\n",
    "dera_regex = re.compile(r' ?/.*$')\n",
    "dera_df = {}\n",
    "\n",
    "from math import floor\n",
    "\n",
    "def generate_intermediate_ddate(df_orig):\n",
    "    if len(df_orig)==1:\n",
    "        return df_orig\n",
    "    year1 = df_orig.iloc[0].ddate\n",
    "    year2 = df_orig.iloc[1].ddate\n",
    "    if (year1-year2).days > 731:\n",
    "        print(\"gap years\")\n",
    "        print(df_orig.iloc[0:2])\n",
    "    new_df = df_orig.iloc[[0]].copy()\n",
    "    year_end = pd.to_datetime(f\"{floor((year1.year+year2.year)/2.0)}1231\", format='%Y%m%d', utc=True)\n",
    "    new_df.ddate = year_end\n",
    "    if year1.year==year2.year:\n",
    "        print(\"same years\")\n",
    "        print(df_orig.iloc[0:2])\n",
    "        new_df.value = (df_orig.iloc[0].value + df_orig.iloc[1].value)/2.0\n",
    "    else:\n",
    "        new_df.value = ((365.0-(year1-year_end).days)*df_orig.iloc[0].value + (year_end-year2).days*df_orig.iloc[1].value)/365.0\n",
    "    new_df.version = df_orig.iloc[0].adsh\n",
    "    print(f\"adding fact ({new_df.tag})\")\n",
    "    return new_df\n",
    "\n",
    "# When this function is called, we already know that we have no matches in FLOAT_TAGS.\n",
    "# GROUPED_DF is grouped by ADSH and only for annual reports.  DDATE can be anything (because many reports look back 1-5 years)\n",
    "# We are working these annual reports quarter by quarter for the quarter in which they are reported\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        df = grouped_df.get_group(key)\n",
    "        # We have no overall float value.  Build from shares outstanding * price\n",
    "        df_shares = df[df.tag.isin(share_tags) & (df.value>0)]\n",
    "        if df_shares.empty:\n",
    "            continue\n",
    "        else:\n",
    "            for share_tag in share_tags:\n",
    "                if not df_shares[df_shares.tag==share_tag].empty:\n",
    "                    df_shares = df_shares[df_shares.tag==share_tag]\n",
    "                    break\n",
    "            if len(df_shares[df_shares.ddate.dt.year==df_shares.fy.dt.year])>1:\n",
    "                print(\"thinning shares\")\n",
    "                df_shares = df_shares[df_shares.ddate.dt.year==df_shares.fy.dt.year]\n",
    "                df_shares = df_shares.sort_values('ddate', ascending=False)\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        # We have no overall price.  Build from price derived from treasury valuation\n",
    "        if df_prices.empty:\n",
    "            df_treasury_shares = df[df.tag.isin(treasury_share_tags)]\n",
    "            df_treasury_value = df[df.tag.isin(treasury_value_tags)]\n",
    "            if df_treasury_shares.empty or df_treasury_value.empty:\n",
    "                continue\n",
    "            df_svp = df_treasury_value.merge(df_treasury_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                print(f\"{df.adsh.iat[0]}: merge failed (1)\")\n",
    "                continue\n",
    "            # Pick latest date / largest number of shares as basis\n",
    "            df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]].copy()\n",
    "            price_per_share = df_float.value_x.squeeze() / df_float.value_y.squeeze()\n",
    "            df_float.rename(columns={'uom_x':'uom'},inplace=True)\n",
    "            tag = 'ComputedTreasuryFloat'\n",
    "        else:\n",
    "            # if df_prices[df_prices.tag.str.startswith('ShareBasedCompensationArrangementByShareBasedPaymentAward')].empty:\n",
    "            #     print(f\"Must use market prices; len(df_prices) =  {len(df_prices)}\")\n",
    "            # else:\n",
    "            #     print(f\"Can use Share Based Comp {df_prices.tag.str[45:]}:\\n{df_prices}\\n\\n\")\n",
    "            # We derive a price from market reports\n",
    "            df_svp = df_prices.merge(df_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                if len (df_prices[df_prices.ddate.dt.year==df_prices.fy.dt.year])>1:\n",
    "                    print(\"thinning prices\")\n",
    "                    df_prices = df_prices[df_prices.ddate.dt.year==df_prices.fy.dt.year]\n",
    "                if len(df_prices)<3 and len(df_shares)<3:\n",
    "                    df_prices = generate_intermediate_ddate(df_prices.sort_values('ddate', ascending=False))\n",
    "                    df_float = generate_intermediate_ddate(df_shares.sort_values('ddate', ascending=False))\n",
    "                    price_per_share = df_prices.value.squeeze()\n",
    "                    # print(\"merge rescued (2)\")\n",
    "                    # display(df_shares)\n",
    "                else:\n",
    "                    print(f\"{df.adsh.iat[0]}: merge failed (2)\")\n",
    "                    print(f\"len(df_prices) = {len(df_prices)}\")\n",
    "                    print(f\"len(df_shares) = {len(df_shares)}\")\n",
    "                    display(df_prices)\n",
    "                    display(df_shares)\n",
    "                    continue\n",
    "            else:\n",
    "                # Pick latest date / largest number of shares as basis\n",
    "                df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]]\n",
    "                price_per_share = df_float.value_x.squeeze() # value_x is a price in this case\n",
    "                df_float.rename(columns={'uom_x':'uom'},inplace=True)\n",
    "            tag = 'ComputedMarketFloat'\n",
    "        df_float = df_float[['adsh', 'ddate', 'uom', 'coreg']]\n",
    "        df_float['tag'] = tag\n",
    "        # TODO: should connect price ddate with total shares ddate\n",
    "        total_shares = df_shares.iloc[0].value\n",
    "        df_float['value'] = price_per_share * total_shares\n",
    "        df_float['qtrs'] = 0\n",
    "        df_float['srcdir'] = 'computed'\n",
    "        df_float['version'] = df_float['footnote'] = pd.NA\n",
    "        df_float = df_float.astype(df.drop(columns=['fy','fp']).dtypes.to_dict())\n",
    "        new_df = new_df.append(df_float)\n",
    "    return new_df\n",
    "\n",
    "def read_dera_table(zf, fy_qtr, tbl):\n",
    "    \"\"\"From a local file ZF, read data for the period FY_QTR for the DERA table TBL.\n",
    "    Return the Dataframe created so that when it is time to create the actual Trino table\n",
    "    we know what the shape of the data should look like.  The returned DF has all the data\n",
    "    of the specific ingestion, not all the data of all the ingestions of data for TBL.\"\"\"\n",
    "    global dera_df\n",
    "    \n",
    "    df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "    df['srcdir'] = fy_qtr\n",
    "    df.srcdir = df.srcdir.astype('string')\n",
    "    \n",
    "    # df = df.convert_dtypes (infer_objects=False, convert_string=True, convert_integer=False, convert_boolean=False, convert_floating=False)\n",
    "    # Print the output\n",
    "    # print(df.dtypes)\n",
    "    \n",
    "    if tbl=='sub':\n",
    "        df.name = df.name.map(lambda x: re.sub(dera_regex, '', x))\n",
    "        df.name = df.name.astype('string')\n",
    "        df['LEI'] = df.name.map(gleif_dict)\n",
    "        df.LEI = df.LEI.astype('string')\n",
    "        df.cik = df.cik.astype('int32')\n",
    "        df.loc[df.sic=='', 'sic'] = pd.NA\n",
    "        df.sic = df.sic.astype('Int16')\n",
    "        df.loc[df.ein=='', 'ein'] = pd.NA\n",
    "        df.ein = df.ein.astype('Int64')\n",
    "        df.wksi = df.wksi.astype('bool')\n",
    "        # df.wksi = df.wksi.astype('int32')\n",
    "        df.period = pd.to_datetime(df.period, format='%Y%m%d', utc=True, errors='coerce')\n",
    "        df.fy = pd.to_datetime(df.fy, format='%Y', utc=True, errors='coerce')\n",
    "        df.filed = pd.to_datetime(df.filed, format='%Y%m%d', utc=True)\n",
    "        df.accepted = pd.to_datetime(df.accepted, format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "        df.prevrpt = df.prevrpt.astype('bool')\n",
    "        df.detail = df.detail.astype('bool')\n",
    "        df.nciks = df.nciks.astype('int16')\n",
    "        \n",
    "        cols = df.columns.tolist()\n",
    "        # Move LEI to a more friendly location in the column order\n",
    "        cols = cols[0:3] + [cols[-1]] + cols[3:-1]\n",
    "        df = df[cols]\n",
    "    elif tbl=='num':\n",
    "        # documentation wrongly lists coreg as NUMERIC length 256.  It is ALPHANUMERIC.\n",
    "        if fy_qtr=='2021q3':\n",
    "            df.loc[df.ddate=='30210630', 'ddate'] = '20210630'\n",
    "        if fy_qtr=='2019q2':\n",
    "            df.loc[df.ddate=='29171231', 'ddate'] = '20171231'\n",
    "        if fy_qtr=='2017q3':\n",
    "            df.loc[df.ddate=='60160630', 'ddate'] = '20160630'\n",
    "        # Fix some bad AES data\n",
    "        if fy_qtr=='2021q1':\n",
    "            df.loc[(df.adsh=='0000874761-21-000015')&(df.tag=='CommonStockValue')&(df.ddate=='20190630'), 'ddate'] = '20200630'\n",
    "        elif fy_qtr=='2020q1':\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='EntityPublicFloat')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='CommonStockValue')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "        df.ddate = pd.to_datetime(df.ddate, format='%Y%m%d', utc=True)\n",
    "        df.qtrs = df.qtrs.astype('int16')\n",
    "        df.loc[df.coreg=='', 'coreg'] = pd.NA\n",
    "        df.loc[df.value=='', 'value'] = pd.NA\n",
    "        df.value = df.value.astype('Float64')\n",
    "        df.loc[df.footnote=='', 'footnote'] = pd.NA\n",
    "        \n",
    "        print(f\"Inferring floats: start len(df) = {len(df)}\")\n",
    "        annual_df = dera_df['sub'][dera_df['sub'].form.isin(['10-K','20-F','40-F'])]\n",
    "        df['fy'] = df.adsh.map(dict(zip(annual_df.adsh,annual_df.fy)))\n",
    "        df['fp'] = df.adsh.map(dict(zip(annual_df.adsh,annual_df.fp)))\n",
    "        print(f\"len(df[df.fp=='FY']) = {len(df[df.fp=='FY'])}\")\n",
    "        df = df[df.fp=='FY']\n",
    "        # df = df.assign(cik=df.adsh.str[:10])\n",
    "        df_has_float = df[df.tag.isin(float_tags)]\n",
    "        print(f\"len(df_has_float) = {len(df_has_float)}\")\n",
    "        df_needs_float = df[~df.adsh.isin(df_has_float.adsh)]\n",
    "        print(f\"len(df_needs_float) = {len(df_needs_float)}\")\n",
    "        float_df = infer_float(df_needs_float[df_needs_float.coreg.isna()\n",
    "                                              &df_needs_float.tag.isin(all_float_helper_tags)\n",
    "                                              &(df_needs_float.value>0)].groupby(['adsh'], as_index=False))\n",
    "        df = df.drop(columns=['fy','fp'])\n",
    "        # print(df.dtypes)\n",
    "        # print(float_df.dtypes)\n",
    "        if float_df.empty:\n",
    "            print(f\"{len(float_df)} floats inferred; Sorry!\")\n",
    "        else:\n",
    "            float_df = float_df.astype(df.dtypes.to_dict())\n",
    "            df = pd.concat([df, float_df])\n",
    "            print(f\"{len(float_df)} floats inferred; {len(float_df[float_df.tag=='ComputedTreasuryFloat'])} treasury-based; {len(float_df[float_df.tag=='ComputedMarketFloat'])} market-based\")\n",
    "    elif tbl=='tag':\n",
    "        df.custom = df.custom.astype('bool')\n",
    "        df.abstract = df.abstract.astype('bool')\n",
    "        df.loc[df.crdr=='', 'crdr'] = pd.NA\n",
    "        df.loc[df.tlabel=='', 'tlabel'] = pd.NA\n",
    "        df.loc[df.doc=='', 'doc'] = pd.NA\n",
    "        df.doc = df.doc.str.replace(':', '\\:')\n",
    "    # print(df.dtypes)\n",
    "    # display(df.head())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e380a6a8-837d-46f1-96a9-c4cd064032b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dera_tables = ['tag', 'sub', 'num']\n",
    "\n",
    "if False:\n",
    "    for tbl in dera_tables:\n",
    "        qres = engine.execute(f\"delete from {ingest_schema}.dera_{tbl}3\")\n",
    "        print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb9abf2-a70b-4341-b14e-22f0f1310340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q4 - tag\n",
      "2017q4 - sub\n",
      "2017q4 - num\n",
      "Inferring floats: start len(df) = 2281152\n",
      "len(df[df.fp=='FY']) = 217585\n",
      "len(df_has_float) = 1047\n",
      "len(df_needs_float) = 2638\n",
      "0001213900-17-012184: merge failed (2)\n",
      "len(df_prices) = 3\n",
      "len(df_shares) = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adsh</th>\n",
       "      <th>tag</th>\n",
       "      <th>version</th>\n",
       "      <th>coreg</th>\n",
       "      <th>ddate</th>\n",
       "      <th>qtrs</th>\n",
       "      <th>uom</th>\n",
       "      <th>value</th>\n",
       "      <th>footnote</th>\n",
       "      <th>srcdir</th>\n",
       "      <th>fy</th>\n",
       "      <th>fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1897396</th>\n",
       "      <td>0001213900-17-012184</td>\n",
       "      <td>SharePrice</td>\n",
       "      <td>us-gaap/2017</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2015-06-30 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>USD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017q4</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>FY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897397</th>\n",
       "      <td>0001213900-17-012184</td>\n",
       "      <td>SharePrice</td>\n",
       "      <td>us-gaap/2017</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2016-06-30 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>USD</td>\n",
       "      <td>1.71</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017q4</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>FY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897398</th>\n",
       "      <td>0001213900-17-012184</td>\n",
       "      <td>SharePrice</td>\n",
       "      <td>us-gaap/2017</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017-06-30 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>USD</td>\n",
       "      <td>0.51</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017q4</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>FY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         adsh         tag       version coreg  \\\n",
       "1897396  0001213900-17-012184  SharePrice  us-gaap/2017  <NA>   \n",
       "1897397  0001213900-17-012184  SharePrice  us-gaap/2017  <NA>   \n",
       "1897398  0001213900-17-012184  SharePrice  us-gaap/2017  <NA>   \n",
       "\n",
       "                            ddate  qtrs  uom  value footnote  srcdir  \\\n",
       "1897396 2015-06-30 00:00:00+00:00     0  USD    6.0     <NA>  2017q4   \n",
       "1897397 2016-06-30 00:00:00+00:00     0  USD   1.71     <NA>  2017q4   \n",
       "1897398 2017-06-30 00:00:00+00:00     0  USD   0.51     <NA>  2017q4   \n",
       "\n",
       "                               fy  fp  \n",
       "1897396 2017-01-01 00:00:00+00:00  FY  \n",
       "1897397 2017-01-01 00:00:00+00:00  FY  \n",
       "1897398 2017-01-01 00:00:00+00:00  FY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adsh</th>\n",
       "      <th>tag</th>\n",
       "      <th>version</th>\n",
       "      <th>coreg</th>\n",
       "      <th>ddate</th>\n",
       "      <th>qtrs</th>\n",
       "      <th>uom</th>\n",
       "      <th>value</th>\n",
       "      <th>footnote</th>\n",
       "      <th>srcdir</th>\n",
       "      <th>fy</th>\n",
       "      <th>fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0001213900-17-012184</td>\n",
       "      <td>EntityCommonStockSharesOutstanding</td>\n",
       "      <td>dei/2014</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017-10-31 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>shares</td>\n",
       "      <td>2411070.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2017q4</td>\n",
       "      <td>2017-01-01 00:00:00+00:00</td>\n",
       "      <td>FY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      adsh                                 tag   version  \\\n",
       "2600  0001213900-17-012184  EntityCommonStockSharesOutstanding  dei/2014   \n",
       "\n",
       "     coreg                     ddate  qtrs     uom      value footnote  \\\n",
       "2600  <NA> 2017-10-31 00:00:00+00:00     0  shares  2411070.0     <NA>   \n",
       "\n",
       "      srcdir                        fy  fp  \n",
       "2600  2017q4 2017-01-01 00:00:00+00:00  FY  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 floats inferred; Sorry!\n",
      "\n",
      "create table if not exists osc_datacommons_dev.sandbox.dera_tag4(\n",
      "    tag varchar,\n",
      "    version varchar,\n",
      "    custom boolean,\n",
      "    abstract boolean,\n",
      "    datatype varchar,\n",
      "    iord varchar,\n",
      "    crdr varchar,\n",
      "    tlabel varchar,\n",
      "    doc varchar,\n",
      "    srcdir varchar\n",
      ") with (\n",
      "    partitioning = array['bucket(tag,20)'],\n",
      "    format = 'ORC'\n",
      ")\n",
      "\n",
      "[(True,)]\n",
      "\n",
      "create table if not exists osc_datacommons_dev.sandbox.dera_sub4(\n",
      "    adsh varchar,\n",
      "    cik integer,\n",
      "    name varchar,\n",
      "    LEI varchar,\n",
      "    sic integer,\n",
      "    countryba varchar,\n",
      "    stprba varchar,\n",
      "    cityba varchar,\n",
      "    zipba varchar,\n",
      "    bas1 varchar,\n",
      "    bas2 varchar,\n",
      "    baph varchar,\n",
      "    countryma varchar,\n",
      "    stprma varchar,\n",
      "    cityma varchar,\n",
      "    zipma varchar,\n",
      "    mas1 varchar,\n",
      "    mas2 varchar,\n",
      "    countryinc varchar,\n",
      "    stprinc varchar,\n",
      "    ein bigint,\n",
      "    former varchar,\n",
      "    changed varchar,\n",
      "    afs varchar,\n",
      "    wksi boolean,\n",
      "    fye varchar,\n",
      "    form varchar,\n",
      "    period timestamp(6),\n",
      "    fy timestamp(6),\n",
      "    fp varchar,\n",
      "    filed timestamp(6),\n",
      "    accepted timestamp(6),\n",
      "    prevrpt boolean,\n",
      "    detail boolean,\n",
      "    instance varchar,\n",
      "    nciks integer,\n",
      "    aciks varchar,\n",
      "    srcdir varchar\n",
      ") with (\n",
      "    partitioning = array['bucket(adsh,20)', 'bucket(cik,20)', 'fy'],\n",
      "    format = 'ORC'\n",
      ")\n",
      "\n",
      "[(True,)]\n",
      "\n",
      "create table if not exists osc_datacommons_dev.sandbox.dera_num4(\n",
      "    adsh varchar,\n",
      "    tag varchar,\n",
      "    version varchar,\n",
      "    coreg varchar,\n",
      "    ddate timestamp(6),\n",
      "    qtrs integer,\n",
      "    uom varchar,\n",
      "    value double,\n",
      "    footnote varchar,\n",
      "    srcdir varchar\n",
      ") with (\n",
      "    partitioning = array['bucket(adsh,20)', 'bucket(tag,20)', 'ddate'],\n",
      "    format = 'ORC'\n",
      ")\n",
      "\n",
      "[(True,)]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "objects=source_bucket.objects.filter(Prefix='SEC-DERA/2017q4')\n",
    "\n",
    "for obj in objects:\n",
    "    if obj.key.endswith('.zip'):\n",
    "        zipfile_src = s3_source.Object(os.environ['S3_LANDING_BUCKET'],obj.key)\n",
    "        tmpname = obj.key.split('/')[-1]\n",
    "        zipfile_src.download_file(f'/tmp/{tmpname}')\n",
    "        zipfile_obj = zipfile.ZipFile(f'/tmp/{tmpname}', mode='r')\n",
    "        fy_qtr = tmpname.split('.')[0]\n",
    "        for tbl in dera_tables:\n",
    "            print(f'{fy_qtr} - {tbl}')\n",
    "            with zipfile_obj.open(f\"{tbl}.txt\") as zf:\n",
    "                # Read data from ZF into a dataframe.\n",
    "                dera_df[tbl] = read_dera_table (zf, fy_qtr, tbl)\n",
    "        zipfile_obj.close()\n",
    "\n",
    "        if False:\n",
    "            # Alas, there is some minor post-fixing we need to do before ingesting into parquet\n",
    "            df = dera_df['num']\n",
    "            num_fields = df.columns\n",
    "            df = df[df.tag=='ComputedTreasuryFloat']\n",
    "            treasury_df = dera_df['sub'].loc[dera_df['sub'].fp=='FY', ['adsh', 'cik', 'name','fye', 'fy', ]].merge(df, on='adsh')\n",
    "            if (len(treasury_df)>0):\n",
    "                display(treasury_df)\n",
    "                grouped_df = treasury_df.groupby('cik')\n",
    "                for key, item in grouped_df:\n",
    "                    if len(item)==1:\n",
    "                        df = item[num_fields].copy()\n",
    "                        df.ddate = pd.to_datetime(f\"{item.fy.squeeze().year}1231\", format='%Y%m%d')\n",
    "                        df.version = item.adsh.squeeze()\n",
    "                        # df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        print(\"adding fact (1)\")\n",
    "                        print(df)\n",
    "                        df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        dera_df['num'] = dera_df['num'].append(df)\n",
    "                    else:\n",
    "                        item = item.sort_values('ddate', ascending=False).reset_index()\n",
    "                        df = generate_intermediate_ddate_value(item.iloc[0:2])\n",
    "                        dera_df['num'] = dera_df['num'].append(df)\n",
    "        if False:\n",
    "            # Now output as parquet files\n",
    "            for tbl in dera_tables:\n",
    "                buf = io.BytesIO()\n",
    "                dera_df[tbl].to_parquet(path=buf)\n",
    "                buf.seek(0)\n",
    "                fy_qtr = dera_df[tbl].iloc[0].srcdir\n",
    "                trino_bucket.upload_fileobj(Fileobj=buf,\n",
    "                                            Key=f'trino/{ingest_schema}/{tbl}/{fy_qtr}.parquet')\n",
    "        else:\n",
    "            # Import into Trino the slow way...\n",
    "            for tbl in dera_tables:\n",
    "                ingest_table = f\"dera_{tbl}4\"\n",
    "                partition = {\"sub\":\"'bucket(adsh,20)', 'bucket(cik,20)', 'fy'\", \"tag\":\"'bucket(tag,20)'\", \"num\":\"'bucket(adsh,20)', 'bucket(tag,20)', 'ddate'\"}[tbl]\n",
    "                columnschema = osc.create_table_schema_pairs(dera_df[tbl],\n",
    "                                                             typemap={\"int16\":\"integer\", \"Int16\":\"integer\",\n",
    "                                                                      \"datetime64[ns, UTC]\":\"timestamp(6)\"})\n",
    "                if partition:\n",
    "                    partition_string = f\"partitioning = array[{partition}],\"\n",
    "                else:\n",
    "                    partition_string = \"\"\n",
    "                tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{ingest_table}(\n",
    "{columnschema}\n",
    ") with (\n",
    "    {partition_string}\n",
    "    format = 'ORC'\n",
    ")\n",
    "\"\"\"\n",
    "                print(tabledef)\n",
    "                qres = engine.execute(tabledef)\n",
    "                print(qres.fetchall())\n",
    " \n",
    "                dera_df[tbl].to_sql(ingest_table,\n",
    "                                    con=engine, schema=ingest_schema, if_exists='append',\n",
    "                                    index=False,\n",
    "                                    method=osc.TrinoBatchInsert(batch_size=1000, verbose = False))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35a95b38-b5a7-4b6f-a454-01ac62e622a4",
   "metadata": {},
   "source": [
    "# Once we have all our parquet files in place, load up the tables with their directory contents\n",
    "for tbl in dera_tables:\n",
    "    if tbl not in dera_df:\n",
    "        error(f'{tbl} data not found')\n",
    "    tabledef = osc.unmanaged_parquet_tabledef(dera_df[tbl],\n",
    "                                              ingest_catalog, ingest_schema, tbl, trino_bucket,\n",
    "                                              typemap={'int16':'smallint', 'Int16':'smallint'})\n",
    "    qres = engine.execute(tabledef)\n",
    "    for row in qres.fetchall():\n",
    "        print(row)\n",
    "\n",
    "    dataset_query = (f'SELECT * FROM {ingest_catalog}.{ingest_schema}.{tbl} limit 10')\n",
    "    print(dataset_query)\n",
    "    dataset = engine.execute(dataset_query)\n",
    "    for row in dataset.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "772604b8-7ba1-475b-8257-831c523c65e0",
   "metadata": {},
   "source": [
    "zipfile_obj = zipfile.ZipFile(f'/tmp/2021q2.zip', mode='r')\n",
    "fy_qtr = tmpname.split('.')[0]\n",
    "for zipinfo in zipfile_obj.infolist():\n",
    "    fname = zipinfo.filename\n",
    "    if fname != 'num.txt':\n",
    "        continue\n",
    "    if fname[3:] != '.txt':\n",
    "        continue\n",
    "    tbl = fname[:3]\n",
    "    if tbl not in dera_tables:\n",
    "        continue\n",
    "    ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "    print(f'{fy_qtr} - {tbl}')\n",
    "    with zipfile_obj.open(fname) as zf:\n",
    "        # This fills a directory with parquet files\n",
    "        df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "        df_shares = df[df.tag.isin(share_tags)]\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        print(len(df_shares))\n",
    "        print(len(df_prices))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "714094ff-06b1-4192-8a2e-664c354d70a5",
   "metadata": {},
   "source": [
    "df.loc[df.value=='', 'value'] = pd.NA\n",
    "df[(df.adsh=='0001193125-21-195161')&(df.tag=='SharesOutstanding')].sort_values('value',ascending=False).iloc[0]['value'] is pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1375a547-4e34-484b-8ff4-b40416be315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrow metadata code from DERA-iceberg if/when we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8285fe0e-e7ce-4c5b-8f2d-d6496e766ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dera_sub4', (6550,)), ('dera_num4', (217585,)), ('dera_tag4', (99490,)), ('ticker', (12421,))]\n"
     ]
    }
   ],
   "source": [
    "tablenames = ['dera_sub4', 'dera_num4', 'dera_tag4', 'ticker']\n",
    "l = []\n",
    "for tbl in tablenames:\n",
    "    qres = engine.execute(f'select count (*) from {ingest_catalog}.{ingest_schema}.{tbl}')\n",
    "    l.append(qres.fetchall()[0])\n",
    "print(list(zip(tablenames, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84392faa-1963-46bb-ba8c-98870c17838f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
