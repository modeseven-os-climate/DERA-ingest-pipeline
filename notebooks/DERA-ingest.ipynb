{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abf0869-f139-411a-9b34-51945c12cade",
   "metadata": {},
   "source": [
    "# Ingest SEC DERA data into Trino pipeline\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf87f-0bd3-41e1-acb8-c2508110c510",
   "metadata": {},
   "source": [
    "Run these in a notebook cell if you need to install onto your nb env\n",
    "\n",
    "```python\n",
    "# 'capture' magic prevents long outputs from spamming your notebook\n",
    "%%capture pipoutput\n",
    "\n",
    "# For loading predefined environment variables from files\n",
    "# Typically used to load sensitive access credentials\n",
    "%pip install python-dotenv\n",
    "\n",
    "# Standard python package for interacting with S3 buckets\n",
    "%pip install boto3\n",
    "\n",
    "# Interacting with Trino and using Trino with sqlalchemy\n",
    "%pip install trino sqlalchemy sqlalchemy-trino\n",
    "\n",
    "# Pandas and parquet file i/o\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# OS-Climate utilities to make data ingest easier\n",
    "%pip install osc-ingest-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789567ff-268c-45e8-8f6f-e768ba62f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "from osc_ingest_trino import *\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7692120-2f4f-48be-9847-0f78a3359bc1",
   "metadata": {},
   "source": [
    "Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba14b4-0746-4ce6-9c0f-ff680221bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff328b-df7c-478c-8a47-a3cd7f03803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "env_var_prefix = 'TRINO'\n",
    "\n",
    "sqlstring = 'trino://{user}@{host}:{port}/'.format(\n",
    "    user = os.environ[f'{env_var_prefix}_USER'],\n",
    "    host = os.environ[f'{env_var_prefix}_HOST'],\n",
    "    port = os.environ[f'{env_var_prefix}_PORT']\n",
    ")\n",
    "sqlargs = {\n",
    "    'auth': trino.auth.JWTAuthentication(os.environ[f'{env_var_prefix}_PASSWD']),\n",
    "    'http_scheme': 'https',\n",
    "    'catalog': 'osc_datacommons_dev'\n",
    "}\n",
    "engine = create_engine(sqlstring, connect_args = sqlargs)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e3f55-c8d5-489e-bb5e-9799ff9f68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osc_ingest_trino import *\n",
    "\n",
    "trino_bucket = attach_s3_bucket(\"S3_DEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fa84b-d4c0-4dd6-99da-2cab1641ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efe080-b4eb-4713-8fd0-7f1d2e1fd966",
   "metadata": {},
   "source": [
    "Open a Trino connection using JWT for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0dce7-1762-4605-8617-f2bf6d649f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_catalog = 'osc_datacommons_dev'\n",
    "ingest_schema = 'sec_dera'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96687d-74a7-46d8-9944-b63fddf11bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available schemas to ensure trino connection is set correctly\n",
    "schema_read = engine.execute(f'show schemas in {ingest_catalog}')\n",
    "for row in schema_read.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d59fd-f47a-4244-9b4a-3cd6d2d1f164",
   "metadata": {},
   "source": [
    "Enter the Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c5cf5-b15e-4ce0-ad90-e14661950f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b412-e27d-4ccf-a65f-0f00b47affe4",
   "metadata": {},
   "source": [
    "Drop previous tables and schema to start with a fresh slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f74df3-3715-4219-8303-48af7f78c9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for view in [ 'assets_by_adsh_ddate', 'assets_by_lei',\n",
    "               'cash_by_adsh_ddate', 'cash_by_lei',\n",
    "               'debt_by_adsh_ddate', 'debt_by_lei',\n",
    "               'financials_by_lei',\n",
    "               'float_by_adsh_ddate', 'float_by_lei',\n",
    "               'fy_revenue_by_lei']:\n",
    "    sql = f\"\"\"\n",
    "drop view if exists {ingest_catalog}.{ingest_schema}.{view}\n",
    "\"\"\"\n",
    "    # print(sql)\n",
    "    qres = engine.execute(sql)\n",
    "    print(qres.fetchall())\n",
    "\n",
    "for ingest_table in [ 'sub', 'num', 'tag', 'ticker',\n",
    "                      't_a', 't_c', 't_d', 't_f', 't_r']:\n",
    "    sql = f\"\"\"\n",
    "drop table if exists {ingest_catalog}.{ingest_schema}.{ingest_table}\n",
    "\"\"\"\n",
    "    # print(sql)\n",
    "    qres = engine.execute(sql)\n",
    "    print(qres.fetchall())\n",
    "\n",
    "qres = engine.execute(f\"show tables in {ingest_schema}\")\n",
    "print(qres.fetchall())\n",
    "\n",
    "sql = f\"\"\"\n",
    "drop schema if exists {ingest_catalog}.{ingest_schema}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdb125-1769-40f8-ace7-6daa29456c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure schema exists, or table creation below will fail in weird ways\n",
    "sql = f\"\"\"\n",
    "create schema {ingest_catalog}.{ingest_schema}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b20942-417b-40c7-a6b0-3bda008b1d98",
   "metadata": {},
   "source": [
    "Load `ticker` file (updated sporadically from https://www.sec.gov/include/ticker.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5495730-2363-42af-b146-7eca5cdb201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'SEC-DERA/ticker.txt')\n",
    "ticker_file.download_file(f'/tmp/dera-ticker.txt')\n",
    "ticker_df = pd.read_csv(f'/tmp/dera-ticker.txt', names=['tname', 'cik'], header=None, sep='\\t', dtype={'tname':'string','cik':'int64'}, engine='c')\n",
    "ticker_dict = dict(zip(ticker_df.cik, ticker_df.tname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade336d-a3f7-4edf-989e-f9e8c0bad982",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f886392-62d5-4f94-abc7-82a5f0d9e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "ticker_df.to_parquet(path=buf)\n",
    "buf.seek(0)\n",
    "trino_bucket.upload_fileobj(Fileobj=buf,\n",
    "                      Key=f'trino/{ingest_schema}/ticker/data.parquet')\n",
    "\n",
    "sql = f\"\"\"\n",
    "drop table if exists {ingest_catalog}.{ingest_schema}.ticker;\n",
    "create table {ingest_catalog}.{ingest_schema}.ticker(\n",
    "    cik bigint,\n",
    "    tname varchar\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{trino_bucket.name}/trino/{ingest_schema}/ticker/'\n",
    ");\n",
    "select count (*) from {ingest_catalog}.{ingest_schema}.ticker;\n",
    "select * from {ingest_catalog}.{ingest_schema}.ticker limit 10\n",
    "\"\"\"\n",
    "for sql_stmt in sql.split(';'):\n",
    "    print(sql_stmt)\n",
    "    qres = engine.execute(sql_stmt)\n",
    "    print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f3bf-da3f-4d85-94ea-0783bebf9ed2",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28874f-b565-4231-927f-f0db6b9364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleif_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'mtiemann-GLEIF/DERA-matches.csv')\n",
    "gleif_file.download_file(f'/tmp/dera-gleif.csv')\n",
    "gleif_df = pd.read_csv(f'/tmp/dera-gleif.csv', header=0, sep=',', dtype=str, engine='c')\n",
    "gleif_dict = dict(zip(gleif_df.name, gleif_df.LEI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902df315-a8f2-43e3-9d1c-f1bdddeae1a9",
   "metadata": {},
   "source": [
    "Load the SUB, NUM, and TAG tables into Trino"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5959517e-cf61-4c01-b236-908726fec3ff",
   "metadata": {},
   "source": [
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "rmi_20211120_b = io.BytesIO(bucket.Object('RMI/RMI-20211120.zip').get()['Body'].read())\n",
    "rmi_20211120_zip = zipfile.ZipFile(rmi_20211120_b, mode='r')\n",
    "del(rmi_20211120_b)\n",
    "# display(zipfile.ZipFile(rmi_20211120_zip, mode='r').filelist)\n",
    "rmi_dd = rmi_20211120_zip.read('data_dictionary.xlsx')\n",
    "\n",
    "# Read all the sheets.  rmi_excel['sheet_name'] gives a specific sheet\n",
    "rmi_20211120_xls = pd.read_excel(rmi_dd, sheet_name=None, dtype=str)\n",
    "\n",
    "del(rmi_dd)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cce65b5a-6fec-47a1-8646-0faffd316513",
   "metadata": {},
   "source": [
    "from io import BytesIO\n",
    "import datetime\n",
    "\n",
    "# The dataframe for Non-Null data\n",
    "df_nn = None\n",
    "\n",
    "rmi_ingest_schemas = { 'rmi_20210929': ('September 2021', rmi_20210929_xls, rmi_20210929_zip),\n",
    "                       'rmi_20211120': ('November 2021', rmi_20211120_xls, rmi_20211120_zip) }\n",
    "\n",
    "# There is no datafile behind the data dictionary.  Run this to prime overview_dd, which all other metadata-finding depends upon\n",
    "for schemaname, (release_date, workbook, zipfile) in rmi_ingest_schemas.items():\n",
    "    overview_meta_fields, overview_meta_content = generate_sheet_meta(workbook, 'Overview', release_date)\n",
    "\n",
    "    for zipinfo in zipfile.infolist():\n",
    "        fname = zipinfo.filename\n",
    "        ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "        if fname[-4:] != '.csv':\n",
    "            continue\n",
    "        \n",
    "        tablename = fname.split('/')[-1].split('.')[0]\n",
    "        # For as-yet unexplained reasons, RMI renamed the file assets_earnings_investments.csv\n",
    "        # without updating the name of the sheet in the spreadsheet.  We switch to the intended name here\n",
    "        if tablename == 'assets_earnings':\n",
    "            tablename = 'assets_earnings_investments'\n",
    "\n",
    "        print(fname)\n",
    "        ## EXAMPLES OF TABLE-SPECIFIC READING FOLLOWS\n",
    "        with zipfile.open(fname) as zf:\n",
    "            if tablename=='state_utility_policies':\n",
    "                df = pd.read_csv(zf, dtype={'respondent_id':'int32'},parse_dates=['date_updated'],dayfirst=True, engine='c')\n",
    "            elif tablename.startswith('utility'):\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], engine='c')\n",
    "            elif tablename=='state_targets':\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], engine='c')\n",
    "                df.year.fillna('-1', inplace=True)\n",
    "                df.loc[df.year.isin(['Annual','2005/1990']), 'year'] = '-1'\n",
    "                df.year = pd.to_datetime(df.year.map(lambda x: x.split('.')[0]).astype('int32'), format='%Y', errors='coerce')\n",
    "            else:\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], thousands=',', engine='c')\n",
    "                if 'year' in df.columns:\n",
    "                    df.year.fillna('-1', inplace=True)\n",
    "                    df.year = pd.to_datetime(df.year.map(lambda x: x.split('.')[0]).astype('int32'), format='%Y', errors='coerce')\n",
    "                df.info(verbose=True)\n",
    "        ## TABLE-SPECIFIC PROCESSING REPLACES THIS COMMENT\n",
    "        create_trino_pipeline (s3_trino, schemaname, tablename, ftimestamp, df, custom_meta_fields, custom_meta_content)\n",
    "    zipfile.close()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6bb495-c034-4e8e-acf1-cbbd03efc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "# Add a unique identifier to the data set\n",
    "uid = str(uuid.uuid4())\n",
    "\n",
    "dera_regex = re.compile(r' ?/.*$')\n",
    "\n",
    "def ingest_dera_table(zf, fy_qtr, tbl, ftimestamp):\n",
    "    \"\"\"From a local file ZF, read data for the period FY_QTR for the DERA table TBL.\n",
    "    Return the Dataframe created so that when it is time to create the actual Trino table\n",
    "    we know what the shape of the data should look like.  The returned DF has all the data\n",
    "    of the specific ingestion, not all the data of all the ingestions of data for TBL.\"\"\"\n",
    "    df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "    df['srcdir'] = fy_qtr\n",
    "    df.srcdir = df.srcdir.astype('string')\n",
    "    df['uuid'] = uid\n",
    "    df.uuid = df.uuid.astype('string')\n",
    "    \n",
    "    # df = df.convert_dtypes (infer_objects=False, convert_string=True, convert_integer=False, convert_boolean=False, convert_floating=False)\n",
    "    # Print the output\n",
    "    # print(df.dtypes)\n",
    "    \n",
    "    if tbl=='sub':\n",
    "        df.name = df.name.map(lambda x: re.sub(dera_regex, '', x))\n",
    "        df.name = df.name.astype('string')\n",
    "        df['LEI'] = df.name.map(gleif_dict)\n",
    "        df.LEI = df.LEI.astype('string')\n",
    "        df.cik = df.cik.astype('int32')\n",
    "        df.loc[df.sic=='', 'sic'] = pd.NA\n",
    "        df.sic = df.sic.astype('Int16')\n",
    "        df.loc[df.ein=='', 'ein'] = pd.NA\n",
    "        df.ein = df.ein.astype('Int64')\n",
    "        df.wksi = df.wksi.astype('bool')\n",
    "        # df.wksi = df.wksi.astype('int32')\n",
    "        df.period = pd.to_datetime(df.period, format='%Y%m%d', utc=True, errors='coerce')\n",
    "        df.fy = pd.to_datetime(df.fy, format='%Y', utc=True, errors='coerce')\n",
    "        df.filed = pd.to_datetime(df.filed, format='%Y%m%d', utc=True)\n",
    "        df.accepted = pd.to_datetime(df.accepted, format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "        df.prevrpt = df.prevrpt.astype('bool')\n",
    "        df.detail = df.detail.astype('bool')\n",
    "        df.nciks = df.nciks.astype('int16')\n",
    "        \n",
    "        cols = df.columns.tolist()\n",
    "        # Move LEI to a more friendly location in the column order\n",
    "        cols = cols[0:3] + [cols[-1]] + cols[3:-1]\n",
    "        df = df[cols]\n",
    "    elif tbl=='num':\n",
    "        # documentation wrongly lists coreg as NUMERIC length 256.  It is ALPHANUMERIC.\n",
    "        df.loc[df.ddate=='30210630', 'ddate'] = '20210630'\n",
    "        df.ddate = pd.to_datetime(df.ddate, format='%Y%m%d', utc=True)\n",
    "        df.qtrs = df.qtrs.astype('int8')\n",
    "        df.loc[df.coreg=='', 'coreg'] = pd.NA\n",
    "        df.loc[df.value=='', 'value'] = pd.NA\n",
    "        df.value = df.value.astype('Float64')\n",
    "        df.loc[df.footnote=='', 'footnote'] = pd.NA\n",
    "    elif tbl=='tag':\n",
    "        df.custom = df.custom.astype('bool')\n",
    "        df.abstract = df.abstract.astype('bool')\n",
    "        df.loc[df.crdr=='', 'crdr'] = pd.NA\n",
    "        df.loc[df.tlabel=='', 'tlabel'] = pd.NA\n",
    "        df.loc[df.doc=='', 'doc'] = pd.NA\n",
    "    # print(df.dtypes)\n",
    "    # display(df.head())\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    df.to_parquet(path=buf)\n",
    "    buf.seek(0)\n",
    "    bucket.upload_fileobj(Fileobj=buf,\n",
    "                          Key=f'trino/{ingest_schema}/{tbl}/{fy_qtr}.parquet')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ab7b7-4f5c-4957-9616-6df5820856cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "objects=bucket.objects.filter(Prefix='SEC-DERA/20')\n",
    "\n",
    "dera_tables = ['sub', 'num', 'tag']\n",
    "dera_df = {}\n",
    "\n",
    "for obj in objects:\n",
    "    if obj.key.endswith('.zip'):\n",
    "        zipfile_src = s3_source.Object(os.environ['S3_LANDING_BUCKET'],obj.key)\n",
    "        tmpname = obj.key.split('/')[-1]\n",
    "        zipfile_src.download_file(f'/tmp/{tmpname}')\n",
    "        zipfile_obj = zipfile.ZipFile(f'/tmp/{tmpname}', mode='r')\n",
    "        fy_qtr = tmpname.split('.')[0]\n",
    "        for zipinfo in zipfile_obj.infolist():\n",
    "            fname = zipinfo.filename\n",
    "            if fname[3:] != '.txt':\n",
    "                continue\n",
    "            tbl = fname[:3]\n",
    "            if tbl not in dera_tables:\n",
    "                continue\n",
    "            ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "            print(f'{fy_qtr} - {tbl}')\n",
    "            with zipfile_obj.open(fname) as zf:\n",
    "                # This fills a directory with parquet files\n",
    "                dera_df[tbl] = ingest_dera_table (zf, fy_qtr, tbl, ftimestamp)\n",
    "        zipfile_obj.close()\n",
    "\n",
    "# Once we have all our parquet files in place, load up the tables with their directory contents\n",
    "for tbl in dera_tables:\n",
    "    if tbl not in dera_df:\n",
    "        error(f'{tbl} data not found')\n",
    "    table_check = engine.execute(f'drop table if exists {ingest_catalog}.{ingest_schema}.{tbl}')\n",
    "    for row in table_check.fetchall():\n",
    "        print(row)\n",
    "\n",
    "    columnschema = create_table_schema_pairs(dera_df[tbl], typemap={'int8':'tinyint', 'int16':'smallint', 'Int16':'smallint'})\n",
    "    tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{tbl} (\n",
    "{columnschema}\n",
    ") with (\n",
    "format = 'parquet',\n",
    "external_location = 's3a://{trino_bucket.name}/trino/{ingest_schema}/{tbl}/'\n",
    ")\n",
    "\"\"\"\n",
    "    print(tabledef)\n",
    "\n",
    "    table_create = engine.execute(tabledef)\n",
    "    for row in table_create.fetchall():\n",
    "        print(row)\n",
    "\n",
    "    dataset_query = (f'SELECT * FROM {ingest_catalog}.{ingest_schema}.{tbl} limit 10')\n",
    "    print(dataset_query)\n",
    "    dataset = engine.execute(dataset_query)\n",
    "    for row in dataset.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375a547-4e34-484b-8ff4-b40416be315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrow metadata code from DERA-iceberg if/when we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285fe0e-e7ce-4c5b-8f2d-d6496e766ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablenames = ['sub', 'num', 'tag', 'ticker']\n",
    "l = []\n",
    "for tbl in tablenames:\n",
    "    qres = engine.execute(f'select count (*) from {ingest_catalog}.{ingest_schema}.{tbl}')\n",
    "    l.append(qres.fetchall()[0])\n",
    "print(list(zip(tablenames, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ec948-4c89-4e7e-96ca-fa3d7c3b4535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
