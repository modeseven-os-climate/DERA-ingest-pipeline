{
 "cells": [
  {
   "cell_type": "raw",
   "id": "629351e6-27cb-463f-aacd-66c0fa3571de",
   "metadata": {},
   "source": [
    "# 001: Share, Price, Revenue -> 314 Market Cap\n",
    "# 002: Revenue, Debt, Float -> 10101.99 Market Cap\n",
    "# 003: Revenue, Price, Share -> 271.8 Market Cap\n",
    "# 004: Revenue, Float, Float2 -> 3141.59 Market Cap\n",
    "\n",
    "df = pd.DataFrame({'adsh' : ['001', '002', '001', '002',\n",
    "                          '001', '002', '003', '003', '003', '004', '004', '004', '005'],\n",
    "                   'sic' : ['4911', '4911', '4911', '4911',\n",
    "                          '4911', '4911', '4911', '4911', '4911', '3210', '3210', '3210', '666'],\n",
    "                   'tag' : ['Shares', 'Revenue', 'Price', 'Debt', 'Revenue', 'Float', 'Revenue', 'Price', 'Shares', 'Revenue', 'Float', 'Float2','Nothing'],\n",
    "                   'ddate' : ['01-01-01', '02-02-02', '01-01-01', '02-02-02', '01-01-01', '02-02-02', '03-03-03', '03-03-03', '03-03-03', '04-04-04', '04-04-04', '04-04-04','05-05-05'],\n",
    "                   'value' : [100, 11.0, 3.14, 2.2, 0.02, 10101.99, 189, 100, 2.718, 1024.48, 2718.28, 3141.59,-1.0]})\n",
    "display(df[df['tag'].isin(['Float'])])\n",
    "\n",
    "# adsh_001[adsh_001['tag']=='Shares']['value'].reset_index() * adsh_001[adsh_001['tag']=='Price']['value'].reset_index()\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        print(key)\n",
    "        df = grouped_df.get_group(key)\n",
    "        df_max = df[df.tag.isin(['Float','Float2'])]\n",
    "        if df_max.empty:\n",
    "            df_max = df[df.tag=='Shares']\n",
    "            if not df_max.empty and not df[df.tag=='Price'].empty:\n",
    "                df_max = df_max.copy()\n",
    "                df_max['value'] = df[df.tag=='Price']['value'].mean() * df_max['value'].squeeze()\n",
    "                df_max['tag'] = 'InferredFloat'\n",
    "        else:\n",
    "            df_max = df_max.sort_values('value', ascending=False).iloc[0].copy()\n",
    "            df_max['tag'] = 'SelectedFloat'\n",
    "        new_df = pd.concat([new_df, df_max])\n",
    "    return new_df\n",
    "\n",
    "x = df.groupby(['adsh', 'ddate']).pipe(infer_float)\n",
    "df = df.append(x)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf0869-f139-411a-9b34-51945c12cade",
   "metadata": {},
   "source": [
    "## Ingest SEC DERA data into Trino pipeline\n",
    "\n",
    "Copyright (C) 2024 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf87f-0bd3-41e1-acb8-c2508110c510",
   "metadata": {},
   "source": [
    "Run these in a notebook cell if you need to install onto your nb env\n",
    "\n",
    "```python\n",
    "# 'capture' magic prevents long outputs from spamming your notebook\n",
    "%%capture pipoutput\n",
    "\n",
    "# Standard python package for interacting with S3 buckets\n",
    "%pip install boto3\n",
    "\n",
    "# Interacting with Trino and using Trino with sqlalchemy\n",
    "%pip install trino sqlalchemy sqlalchemy-trino\n",
    "\n",
    "# Pandas and parquet file i/o\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# OS-Climate utilities to make data ingest easier\n",
    "%pip install osc-ingest-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9195b7-6619-4447-b325-81702e7f8e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sqlalchemy import text\n",
    "import io\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "import boto3\n",
    "\n",
    "import osc_ingest_trino as osc\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549acf5-c82c-4c29-bd5e-65ae43532f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('date')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c574ae8-e4f0-413f-b07d-1f193c55384c",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca30b96-6fc4-4ed0-a235-9b599bf3a4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "# Load environment variables from credentials.env\n",
    "osc.load_credentials_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1b0f4-8b05-4e87-9c2a-26b2c5a1e5b0",
   "metadata": {},
   "source": [
    "Open a Trino connection using JWT for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaacc26-b431-42ee-9587-fbcc3941f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_catalog = 'osc_datacommons_dev'\n",
    "iceberg_schema = 'dera'\n",
    "dera_table_prefix = 'dera_'\n",
    "\n",
    "engine = osc.attach_trino_engine(verbose=True, catalog=iceberg_catalog, schema=iceberg_schema)\n",
    "cxn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab1dd2-3242-404c-aace-f6288c185b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available schemas to ensure trino connection is set correctly\n",
    "schema_read = cxn.execute(text(f'show schemas in {iceberg_catalog}'))\n",
    "if schema_read.returns_rows:\n",
    "    for row in schema_read.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af246c5-408a-45f7-b9b7-f935cf3cc1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bucket must be configured with credentials for trino, and accessible to the hive catalog\n",
    "# You may need to use a different prefix here depending on how you name your credentials.env variables\n",
    "hive_bucket = osc.attach_s3_bucket('S3_HIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c04d6-1864-41e7-a754-b33a859274bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create demo table named per user\n",
    "# avoids problems with users reusing table names and associated permission problems\n",
    "uniq = os.environ['TRINO_USER']\n",
    "uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afffd21-4507-46d6-b1c1-06bc9dea2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_catalog = 'osc_datacommons_hive_ingest'\n",
    "hive_schema = 'ingest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cef25-ac08-48cd-ae8e-b8a1e5b828aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_create = osc._do_sql(f\"\"\"\n",
    "create schema if not exists osc_datacommons_dev.{iceberg_schema}\n",
    "WITH (\n",
    "    location = 's3a://osc-datacommons-s3-bucket-dev02/data/{iceberg_schema}.db/'\n",
    ")\n",
    "\"\"\", engine, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fa84b-d4c0-4dd6-99da-2cab1641ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "source_bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b412-e27d-4ccf-a65f-0f00b47affe4",
   "metadata": {},
   "source": [
    "Drop previous tables and schema to start with a fresh slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34233f3-e80d-443b-83ae-968cb899bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"show tables in {iceberg_schema}\"\n",
    "qres = osc._do_sql(sql, engine, verbose=True)\n",
    "print(qres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7760b05-2217-4776-8697-e5551fdf1cd0",
   "metadata": {},
   "source": [
    "Initialize DBT disctionary we will write out as YML at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414896ad-eb5d-47a2-9f30-dd4f9c267a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_dict = {}\n",
    "dbt_dict['models'] = {}\n",
    "\n",
    "repo_root = Path().resolve().parent\n",
    "models_dir = repo_root.joinpath(\"dbt\", \"dera_transform\", \"models\")\n",
    "\n",
    "shutil.rmtree(models_dir, ignore_errors=True)\n",
    "os.mkdir(models_dir, mode=0o755)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31362be-b48a-4e55-a064-b5a9dfd679cc",
   "metadata": {},
   "source": [
    "Load `ticker` file (updated sporadically from https://www.sec.gov/include/ticker.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94deb145-7dd9-443c-a93d-38b0e82153ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"/tmp/dera-ticker.txt\"):\n",
    "    ticker_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'SEC-DERA/ticker.txt')\n",
    "    ticker_file.download_file(\"/tmp/dera-ticker.txt\")\n",
    "ticker_df = pd.read_csv(\"/tmp/dera-ticker.txt\", names=['tname', 'cik'], header=None, sep='\\t', dtype={'tname':'string','cik':'int64'}, engine='c')\n",
    "ticker_dict = dict(zip(ticker_df.cik, ticker_df.tname))\n",
    "\n",
    "ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef96224-f64f-4f61-9c80-37acdc8cb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully spell out catalog, schema, and table because this is source code for the dbt environment\n",
    "ticker_table = 'ticker'\n",
    "\n",
    "with open(models_dir.joinpath(f\"{ticker_table}.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    print(f\"writing to model file {ticker_table}.sql in directory {models_dir}\")\n",
    "    print(\"{{ config(materialized='view', view_security='invoker') }}\" + f\"\"\"\n",
    "with source_data as (\n",
    "    select {', '.join(ticker_df.columns)}\n",
    "    from {iceberg_catalog}.{iceberg_schema}.{ticker_table}_source\n",
    ")\n",
    "select * from source_data\n",
    "\"\"\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf7d46-e501-4b9e-b321-a02d78d2bed6",
   "metadata": {},
   "source": [
    "The following text describes DBT model properties"
   ]
  },
  {
   "cell_type": "raw",
   "id": "344ec1d2-9b26-41b7-93db-569cbe339e60",
   "metadata": {},
   "source": [
    "version: 2\n",
    "\n",
    "models:\n",
    "  - [name](model_name): <model name>\n",
    "    [description](description): <markdown_string>\n",
    "    [docs](resource-properties/docs):\n",
    "      show: true | false\n",
    "    [config](resource-properties/config):\n",
    "      [<model_config>](model-configs): <config_value>\n",
    "    [tests](resource-properties/tests):\n",
    "      - <test>\n",
    "      - ... # declare additional tests\n",
    "    columns:\n",
    "      - name: <column_name> # required\n",
    "        [description](description): <markdown_string>\n",
    "        [meta](meta): {<dictionary>}\n",
    "        [quote](quote): true | false\n",
    "        [tests](resource-properties/tests):\n",
    "          - <test>\n",
    "          - ... # declare additional tests\n",
    "        [tags](resource-configs/tags): [<string>]\n",
    "\n",
    "      - name: ... # declare properties of additional columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b1c4b-20ea-4ccd-9da6-89c039c43704",
   "metadata": {},
   "source": [
    "The following text describes DBT external properties"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1842868d-824a-449e-9a93-dd1fece5fea3",
   "metadata": {},
   "source": [
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: <source_name>\n",
    "    tables:\n",
    "      - name: <table_name>\n",
    "        external:\n",
    "          location: <string>\n",
    "          file_format: <string>\n",
    "          row_format: <string>\n",
    "          tbl_properties: <string>      \n",
    "          partitions:\n",
    "            - name: <column_name>\n",
    "              data_type: <string>\n",
    "              description: <string>\n",
    "              meta: {dictionary}\n",
    "            - ...\n",
    "          <additional_property>: <additional_value>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687663e-dbf4-4f84-9b74-4177f86c4aeb",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "Due to the fact that we need to drop and reload sub, num, and tag just to get the tags right,\n",
    "we drop the views here that depend on those tables, recreating them once the tables are stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992a5cb-b948-4d6b-92d6-e68efcc2c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: right now incremental logic is essentially manual.\n",
    "# Would be nice to cleanly update data on quarterly basis without monkey-patching here\n",
    "\n",
    "incremental = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c400b89-2c58-4254-814b-a7f1878f90a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not incremental:\n",
    "    for view in [ 'assets_by_lei', 'assets_usd_by_lei', 'assets_xyz_by_lei',\n",
    "                  'cash_by_adsh_ddate', 'cash_by_lei', 'cash_usd_by_lei', 'cash_xyz_by_lei',\n",
    "                  'debt_by_adsh_ddate', 'debt_by_lei', 'debt_usd_by_lei', 'debt_xyz_by_lei',\n",
    "                  'financials_by_lei',\n",
    "                  'float_by_lei', 'float_usd_by_lei', 'float_xyz_by_lei',\n",
    "                  'fy_revenue_by_lei', 'fy_revenue_usd_by_lei', 'fy_revenue_xyz_by_lei',\n",
    "                  'fy_income_by_lei', 'fy_income_usd_by_lei', 'fy_income_xyz_by_lei',\n",
    "                ]:\n",
    "        sql = f\"drop view if exists {view}\"\n",
    "        qres = osc._do_sql(sql, engine, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2fb94-cc2a-4484-af35-c3b46d3c014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnschema = osc.create_table_schema_pairs(ticker_df)\n",
    "\n",
    "qres = osc._do_sql(f\"drop table if exists {ticker_table}_source\", engine, verbose=True)\n",
    "\n",
    "tabledef = f\"\"\"\n",
    "create table if not exists {ticker_table}_source(\n",
    "{columnschema}\n",
    ") with (\n",
    "partitioning = array['bucket(tname,10)'],\n",
    "format = 'ORC'\n",
    ")\n",
    "\"\"\"\n",
    "qres = osc._do_sql(tabledef, engine, verbose=True)\n",
    "ticker_df.to_sql(f\"{ticker_table}_source\",\n",
    "                 con=engine, schema=iceberg_schema, if_exists='append',\n",
    "                 index=False,\n",
    "                 method=osc.TrinoBatchInsert(batch_size = 15000, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f3bf-da3f-4d85-94ea-0783bebf9ed2",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28874f-b565-4231-927f-f0db6b9364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleif_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'mtiemann-GLEIF/DERA-matches.csv')\n",
    "gleif_file.download_file(f'/tmp/dera-gleif.csv')\n",
    "gleif_df = pd.read_csv(f'/tmp/dera-gleif.csv', header=0, sep=',', dtype=str, engine='c')\n",
    "gleif_dict = dict(zip(gleif_df.name, gleif_df.LEI))\n",
    "\n",
    "# Manual fixups discovered since processing\n",
    "gleif_dict['GROUP SIMEC SA DE CV'] = '529900LCYCXPA0TZEU09'\n",
    "gleif_dict['ENEL GENERACION CHILE S.A.'] = '549300PVHXUFEIE6LY50'\n",
    "gleif_dict['POSCO HOLDINGS INC.'] = '988400E5HRVX81AYLM04'\n",
    "gleif_dict['ARCHAEA ENERGY INC.'] = '549300ZBE567NNMH7V89'\n",
    "gleif_dict['CLEANSPARK, INC.'] = '254900VO7KBRJQDGY810'\n",
    "gleif_dict['ALGOMA STEEL GROUP INC.'] = '549300Q5EU337A1XCX27'\n",
    "gleif_dict['ECO WAVE POWER GLOBAL AB (PUBL)'] = '5493003GP1XAFTYRJM76'\n",
    "gleif_dict['CPFL ENERGY INC'] = '529900GBWSBDXN8GGM28'\n",
    "gleif_dict['PAMPA ENERGY INC.'] = '254900QNIK0CVURGML24'\n",
    "gleif_dict['ENERGY CO OF MINAS GERAIS'] = '254900W703PXLDSEM056'\n",
    "gleif_dict['BRAZILIAN ELECTRIC POWER CO'] = '254900I8KYDELP4B4Z08'\n",
    "\n",
    "# And in the steel portfolio\n",
    "gleif_dict['FRIEDMAN INDUSTRIES INC'] = '549300VI5ADYNC8C3G47'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902df315-a8f2-43e3-9d1c-f1bdddeae1a9",
   "metadata": {},
   "source": [
    "Helper functions to load the SUB, NUM, and TAG tables into Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d6d11-b078-46ac-9a5b-4c81eca9878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import uuid\n",
    "\n",
    "# Borrowed/stole this definition from SEC Corp Financials notebook...\n",
    "float_tags = [\n",
    "    'EntityPublicFloat',\n",
    "    'StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest',\n",
    "    'FreeFloat',\n",
    "    'PublicFloat',\n",
    "    'PublicFloatValue',\n",
    "]\n",
    "\n",
    "# These are in priority preference order\n",
    "share_tags = [\n",
    "    'EntityCommonStockSharesOutstanding',\n",
    "    'CommonStockSharesOutstanding',\n",
    "    'SharesOutstanding',\n",
    "    'WeightedAverageNumberOfDilutedSharesOutstanding',\n",
    "    'WeightedAverageNumberOfSharesOutstandingBasic',\n",
    "]\n",
    "\n",
    "# We don't want anything that reports par value, such as CommonStockValue\n",
    "shareprice_tags = [\n",
    "    'SharePrice',\n",
    "    'PerSharePrice',\n",
    "    'MarketValuePerShare',\n",
    "    'SaleOfStockPricePerShare',\n",
    "    'CashPricePerOrdinaryShare',\n",
    "    'TreasurySharesValuePerShare',\n",
    "    'SharesOutstandingPricePerShare',\n",
    "]\n",
    "\n",
    "treasury_share_tags = [\n",
    "    'TreasuryStockShares',\n",
    "    'TreasuryStockShares1',\n",
    "]\n",
    "\n",
    "treasury_value_tags = [\n",
    "    'TreasurySharesMarketValue',\n",
    "    'FairValueOfTreasuryShares',\n",
    "    'MarketValueOfTreasuryShares',\n",
    "]\n",
    "\n",
    "all_float_helper_tags = share_tags + shareprice_tags + treasury_share_tags + treasury_value_tags\n",
    "\n",
    "dera_regex = re.compile(r' ?/.*$')\n",
    "dera_df = {}\n",
    "\n",
    "from math import floor\n",
    "\n",
    "def generate_intermediate_ddate(df_orig):\n",
    "    if len(df_orig)==1:\n",
    "        return df_orig\n",
    "    year1 = df_orig.iloc[0].ddate\n",
    "    year2 = df_orig.iloc[1].ddate\n",
    "    if (year1-year2).days > 731:\n",
    "        print(\"gap years\")\n",
    "        print(df_orig.iloc[0:2])\n",
    "    new_df = df_orig.iloc[[0]].copy()\n",
    "    year_end = pd.to_datetime(f\"{floor((year1.year+year2.year)/2.0)}1231\", format='%Y%m%d', utc=True)\n",
    "    new_df.ddate = year_end\n",
    "    if year1.year==year2.year:\n",
    "        print(\"same years\")\n",
    "        print(df_orig.iloc[0:2])\n",
    "        new_df.value = (df_orig.iloc[0].value + df_orig.iloc[1].value)/2.0\n",
    "    else:\n",
    "        new_df.value = ((365.0-(year1-year_end).days)*df_orig.iloc[0].value + (year_end-year2).days*df_orig.iloc[1].value)/365.0\n",
    "    new_df.version = df_orig.iloc[0].adsh\n",
    "    print(f\"adding fact ({new_df.tag})\")\n",
    "    return new_df\n",
    "\n",
    "# When this function is called, we already know that we have no matches in FLOAT_TAGS.\n",
    "# GROUPED_DF is grouped by ADSH and only for annual reports.  DDATE can be anything (because many reports look back 1-5 years)\n",
    "# We are working these annual reports quarter by quarter for the quarter in which they are reported\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        df = grouped_df.get_group(key)\n",
    "        # We have no overall float value.  Build from shares outstanding * price\n",
    "        df_shares = df[df.tag.isin(share_tags) & df.value.gt(0)]\n",
    "        if df_shares.empty:\n",
    "            continue\n",
    "        else:\n",
    "            for share_tag in share_tags:\n",
    "                if not df_shares[df_shares.tag==share_tag].empty:\n",
    "                    df_shares = df_shares[df_shares.tag==share_tag]\n",
    "                    break\n",
    "            if len(df_shares[df_shares.ddate.dt.year==df_shares.fy.dt.year])>1:\n",
    "                print(\"thinning shares\")\n",
    "                df_shares = df_shares[df_shares.ddate.dt.year==df_shares.fy.dt.year]\n",
    "                df_shares = df_shares.sort_values('ddate', ascending=False)\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        # We have no overall price.  Build from price derived from treasury valuation\n",
    "        if df_prices.empty:\n",
    "            df_treasury_shares = df[df.tag.isin(treasury_share_tags)]\n",
    "            df_treasury_value = df[df.tag.isin(treasury_value_tags)]\n",
    "            if df_treasury_shares.empty or df_treasury_value.empty:\n",
    "                continue\n",
    "            df_svp = df_treasury_value.merge(df_treasury_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                print(f\"{df.adsh.iat[0]}: merge failed (1)\")\n",
    "                continue\n",
    "            # Pick latest date / largest number of shares as basis\n",
    "            df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]].copy()\n",
    "            price_per_share = df_float.value_x.squeeze() / df_float.value_y.squeeze()\n",
    "            df_float.rename(columns={'uom_x':'uom'},inplace=True)\n",
    "            tag = 'ComputedTreasuryFloat'\n",
    "        else:\n",
    "            # if df_prices[df_prices.tag.str.startswith('ShareBasedCompensationArrangementByShareBasedPaymentAward')].empty:\n",
    "            #     print(f\"Must use market prices; len(df_prices) =  {len(df_prices)}\")\n",
    "            # else:\n",
    "            #     print(f\"Can use Share Based Comp {df_prices.tag.str[45:]}:\\n{df_prices}\\n\\n\")\n",
    "            # We derive a price from market reports\n",
    "            df_svp = df_prices.merge(df_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                if len (df_prices[df_prices.ddate.dt.year==df_prices.fy.dt.year])>1:\n",
    "                    print(\"thinning prices\")\n",
    "                    df_prices = df_prices[df_prices.ddate.dt.year==df_prices.fy.dt.year]\n",
    "                if len(df_prices)<3 and len(df_shares)<3:\n",
    "                    df_prices = generate_intermediate_ddate(df_prices.sort_values('ddate', ascending=False))\n",
    "                    df_float = generate_intermediate_ddate(df_shares.sort_values('ddate', ascending=False))\n",
    "                    price_per_share = df_prices.value.squeeze()\n",
    "                    # print(\"merge rescued (2)\")\n",
    "                    # display(df_shares)\n",
    "                else:\n",
    "                    print(f\"{df.adsh.iat[0]}: merge failed (2)\")\n",
    "                    print(f\"len(df_prices) = {len(df_prices)}\")\n",
    "                    print(f\"len(df_shares) = {len(df_shares)}\")\n",
    "                    display(df_prices)\n",
    "                    display(df_shares)\n",
    "                    continue\n",
    "            else:\n",
    "                # Pick latest date / largest number of shares as basis\n",
    "                df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]]\n",
    "                price_per_share = df_float.value_x.squeeze() # value_x is a price in this case\n",
    "                df_float.rename(columns={'uom_x':'uom'},inplace=True)\n",
    "            tag = 'ComputedMarketFloat'\n",
    "        df_float = df_float[['adsh', 'ddate', 'uom', 'coreg']]\n",
    "        df_float['tag'] = tag\n",
    "        # TODO: should connect price ddate with total shares ddate\n",
    "        total_shares = df_shares.iloc[0].value\n",
    "        df_float['value'] = price_per_share * total_shares\n",
    "        df_float['qtrs'] = 0\n",
    "        df_float['srcdir'] = 'computed'\n",
    "        df_float['version'] = 'osc_dera_ingest'\n",
    "        df_float['footnote'] = pd.NA\n",
    "        df_float = df_float.astype(df.drop(columns=['fy','fp']).dtypes.to_dict())\n",
    "        new_df = pd.concat([new_df, df_float])\n",
    "    return new_df\n",
    "\n",
    "def read_dera_table(zf, fy_qtr, tbl):\n",
    "    \"\"\"From a local file ZF, read data for the period FY_QTR for the DERA table TBL.\n",
    "    Return the Dataframe created so that when it is time to create the actual Trino table\n",
    "    we know what the shape of the data should look like.  The returned DF has all the data\n",
    "    of the specific ingestion, not all the data of all the ingestions of data for TBL.\"\"\"\n",
    "    global dera_df\n",
    "\n",
    "    df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "    df['srcdir'] = fy_qtr\n",
    "    df.srcdir = df.srcdir.astype('string')\n",
    "\n",
    "    # df = df.convert_dtypes (infer_objects=False, convert_string=True, convert_integer=False, convert_boolean=False, convert_floating=False)\n",
    "    # Print the output\n",
    "    # print(df.dtypes)\n",
    "\n",
    "    if tbl=='sub':\n",
    "        # While the documentation of the DERA SUB table says that FY must be non-NULL, many forms don't specify a fiscal year\n",
    "        # (forms 1, 2, 3, 4, 6, 8, 10-12, POS AM, POS EX, 425, etc).  And none of those forms relate to data we are trying to ingest,\n",
    "        # so we drop them.\n",
    "        df = df[df.fy.ne('') & df.period.ne('')]\n",
    "        df.name = df.name.map(lambda x: re.sub(dera_regex, '', x))\n",
    "        df.name = df.name.astype('string')\n",
    "        df['LEI'] = df.name.map(gleif_dict)\n",
    "        df.LEI = df.LEI.astype('string')\n",
    "        df.cik = df.cik.astype('int32')\n",
    "        df.loc[df.sic=='', 'sic'] = pd.NA\n",
    "        df.sic = df.sic.astype('Int16')\n",
    "        df.loc[df.ein=='', 'ein'] = pd.NA\n",
    "        df.ein = df.ein.astype('Int64')\n",
    "        df.wksi = df.wksi.astype('bool')\n",
    "        # df.wksi = df.wksi.astype('int32')\n",
    "        df.period = pd.to_datetime(df.period, format='%Y%m%d', utc=True, errors='coerce')\n",
    "        df.fy = pd.to_datetime(df.fy, format='%Y', utc=True) # errors='coerce'\n",
    "        df.filed = pd.to_datetime(df.filed, format='%Y%m%d', utc=True)\n",
    "        df.accepted = pd.to_datetime(df.accepted, utc=True)  # format='%Y-%m-%d %H:%M:%S' but sometimes format='%Y-%m-%d %H:%M:%S.%f'\n",
    "        df.prevrpt = df.prevrpt.astype('bool')\n",
    "        df.detail = df.detail.astype('bool')\n",
    "        df.nciks = df.nciks.astype('int16')\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        # Move LEI to a more friendly location in the column order\n",
    "        cols = cols[0:3] + [cols[-1]] + cols[3:-1]\n",
    "        df = df[cols]\n",
    "    elif tbl=='num':\n",
    "        # documentation wrongly lists coreg as NUMERIC length 256.  It is ALPHANUMERIC.\n",
    "        if fy_qtr=='2017q1':\n",
    "            df.loc[df.ddate=='21051130', 'ddate'] = '20151130'\n",
    "            df.loc[df.ddate=='21061130', 'ddate'] = '20161130'\n",
    "        elif fy_qtr=='2017q3':\n",
    "            df.loc[df.ddate=='60160630', 'ddate'] = '20160630'\n",
    "        elif fy_qtr=='2018q2':\n",
    "            df.loc[df.ddate=='22011231', 'ddate'] = '20211231'\n",
    "        elif fy_qtr=='2019q1':\n",
    "            df.loc[df.ddate=='21080430', 'ddate'] = '20180430'\n",
    "            df.loc[df.ddate=='21081031', 'ddate'] = '20181031'\n",
    "        elif fy_qtr=='2019q2':\n",
    "            df.loc[df.ddate=='29171231', 'ddate'] = '20171231'\n",
    "        elif fy_qtr=='2021q3':\n",
    "            df.loc[df.ddate=='30210630', 'ddate'] = '20210630'\n",
    "        elif fy_qtr=='2022q1':\n",
    "            df.loc[df.ddate=='21211231', 'ddate'] = '20211231'\n",
    "        elif fy_qtr=='2024q1':\n",
    "            df.loc[df.ddate=='29230930', 'ddate'] = '20230930'\n",
    "            # Drop specific entries that are duplicated for both the dates 20231231 and 29231231\n",
    "            # We drop the bad year, keeping the good.  Following, we change remaining bad years to good\n",
    "            df = df[df.adsh.ne('0001654954-24-003228')|df.ddate.ne('29231231')|~df.tag.isin(['OperatingLeaseLiabilityCurrent', 'OperatingLeaseRightOfUseAsset'])]\n",
    "            df.loc[df.ddate=='29231231', 'ddate'] = '20231231'\n",
    "\n",
    "        # Fix some bad AES data\n",
    "        if fy_qtr=='2021q1':\n",
    "            df.loc[(df.adsh=='0000874761-21-000015')&(df.tag=='CommonStockValue')&(df.ddate=='20190630'), 'ddate'] = '20200630'\n",
    "        if fy_qtr=='2020q1':\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='EntityPublicFloat')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='CommonStockValue')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "        df.ddate = pd.to_datetime(df.ddate, format='%Y%m%d', utc=True)\n",
    "        df.qtrs = df.qtrs.astype('int16')\n",
    "        df.loc[df.coreg=='', 'coreg'] = pd.NA\n",
    "        df.loc[df.value=='', 'value'] = pd.NA\n",
    "        df.value = df.value.astype('Float64')\n",
    "        df.loc[df.footnote=='', 'footnote'] = pd.NA\n",
    "\n",
    "        print(f\"Inferring floats: start len(df) = {len(df)}\")\n",
    "        annual_df = dera_df['sub'][dera_df['sub'].form.isin(['10-K','20-F','40-F'])]\n",
    "        df['fy'] = df.adsh.map(dict(zip(annual_df.adsh,annual_df.fy)))\n",
    "        df['fp'] = df.adsh.map(dict(zip(annual_df.adsh,annual_df.fp)))\n",
    "        print(f\"len(df[df.fp=='FY']) = {len(df[df.fp=='FY'])}\")\n",
    "        df = df[df.fp=='FY']\n",
    "        # df = df.assign(cik=df.adsh.str[:10])\n",
    "        df_has_float = df[df.tag.isin(float_tags)]\n",
    "        print(f\"len(df_has_float) = {len(df_has_float)}\")\n",
    "        df_needs_float = df[~df.adsh.isin(df_has_float.adsh)]\n",
    "        print(f\"len(df_needs_float) = {len(df_needs_float)}\")\n",
    "        float_df = infer_float(df_needs_float[df_needs_float.coreg.isna()\n",
    "                                              &df_needs_float.tag.isin(all_float_helper_tags)\n",
    "                                              &(df_needs_float.value>0)].groupby(['adsh'], as_index=False))\n",
    "        df = df.drop(columns=['fy','fp'])\n",
    "        # print(df.dtypes)\n",
    "        # print(float_df.dtypes)\n",
    "        if float_df.empty:\n",
    "            print(f\"{len(float_df)} floats inferred; Sorry!\")\n",
    "        else:\n",
    "            float_df = float_df.astype(df.dtypes.to_dict())\n",
    "            df = pd.concat([df, float_df])\n",
    "            print(f\"{len(float_df)} floats inferred; {len(float_df[float_df.tag=='ComputedTreasuryFloat'])} treasury-based; {len(float_df[float_df.tag=='ComputedMarketFloat'])} market-based\")\n",
    "    elif tbl=='tag':\n",
    "        df.custom = df.custom.astype('bool')\n",
    "        df.abstract = df.abstract.astype('bool')\n",
    "        df.loc[df.crdr=='', 'crdr'] = pd.NA\n",
    "        df.loc[df.tlabel=='', 'tlabel'] = pd.NA\n",
    "        df.loc[df.doc=='', 'doc'] = pd.NA\n",
    "    # print(df.dtypes)\n",
    "    # display(df.head())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211435a8-59b9-42e5-a352-09449ef294c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_to_trino_table_with_dbt_metadata(tablename, df, partition_columns=[], custom_meta_content='', custom_meta_fields='', verbose=False):\n",
    "    global engine, models_dir\n",
    "    iceberg_table = f'{dera_table_prefix}{tablename}'\n",
    "\n",
    "    if custom_meta_content:\n",
    "        dbt_models = dbt_dict['models']\n",
    "        dbt_models[iceberg_table] = dbt_table = { 'description': custom_meta_content['description']}\n",
    "        if custom_meta_fields:\n",
    "            dbt_table['columns'] = dbt_columns = (\n",
    "                { name: {'description': custom_meta_fields[name]['Description'] } for name in custom_meta_fields.keys() }\n",
    "            )\n",
    "            for name in custom_meta_fields.keys():\n",
    "                if 'tags' in custom_meta_fields[name].keys():\n",
    "                    dbt_columns[name]['tags'] = custom_meta_fields[name]['tags']\n",
    "    elif custom_meta_fields:\n",
    "        raise VALUE_ERROR\n",
    "\n",
    "    osc.fast_pandas_ingest_via_hive(\n",
    "        df,\n",
    "        engine,\n",
    "        iceberg_catalog, iceberg_schema, f\"{iceberg_table}_source\",\n",
    "        hive_bucket, hive_catalog, hive_schema,\n",
    "        partition_columns = partition_columns,\n",
    "        overwrite = False,\n",
    "        typemap={\"datetime64[ns]\":\"timestamp(6)\", \"datetime64[ns, UTC]\":\"timestamp(6)\",\n",
    "                 \"Int16\":\"integer\", \"int16\":\"integer\"},\n",
    "        verbose = verbose\n",
    "    )\n",
    "\n",
    "    # Fully spell out catalog, schema, and table because this is source code for the dbt environment\n",
    "    with open(models_dir.joinpath(f\"{iceberg_table}.sql\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        if verbose:\n",
    "            print(f\"writing to model file {iceberg_table}.sql in directory {models_dir}\")\n",
    "        print(\"{{ config(materialized='view', view_security='invoker') }}\" + f\"\"\"\n",
    "with source_data as (\n",
    "    select {', '.join(df.columns)}\n",
    "    from {iceberg_catalog}.{iceberg_schema}.{iceberg_table}_source\n",
    ")\n",
    "select * from source_data\n",
    "\"\"\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825dd4cb-7510-4b1a-83f5-d1fd1319d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dera_tablenames = ['tag', 'sub', 'num']\n",
    "\n",
    "if not incremental:\n",
    "    for tbl in dera_tablenames:\n",
    "        qres = osc._do_sql(f\"drop table if exists {dera_table_prefix}{tbl}_source\", engine, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522e20c-86cd-49d6-ad07-3f2aba8b5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects=source_bucket.objects.filter(Prefix='SEC-DERA/20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b124264-2fd9-4456-ba96-3f05b4a5f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dera_tag = pd.DataFrame()\n",
    "batch_size = {'num':1500,'sub':1500,'tag':1000}\n",
    "\n",
    "for obj in objects:\n",
    "    if incremental:\n",
    "        # Monkey-patch here if you want to incrementally load objects\n",
    "        if obj.key != 'SEC-DERA/2024q1.zip':\n",
    "            continue\n",
    "        print(f\"loading {obj.key}\")\n",
    "    if obj.key.endswith('.zip'):\n",
    "        zipfile_src = s3_source.Object(os.environ['S3_LANDING_BUCKET'],obj.key)\n",
    "        tmpname = obj.key.split('/')[-1]\n",
    "        zipfile_src.download_file(f'/tmp/{tmpname}')\n",
    "        zipfile_obj = zipfile.ZipFile(f'/tmp/{tmpname}', mode='r')\n",
    "        fy_qtr = tmpname.split('.')[0]\n",
    "        for tbl in dera_tablenames:\n",
    "            print(f'{fy_qtr} - {tbl}')\n",
    "            with zipfile_obj.open(f\"{tbl}.txt\") as zf:\n",
    "                # Read data from ZF into a dataframe.\n",
    "                dera_df[tbl] = read_dera_table (zf, fy_qtr, tbl)\n",
    "                if tbl == 'tag':\n",
    "                    dera_tag = pd.concat([dera_tag, dera_df[tbl]]).drop_duplicates(subset=['tag','version'])\n",
    "        zipfile_obj.close()\n",
    "\n",
    "        if True:\n",
    "            # Alas, there is some minor post-fixing we need to do before ingesting into parquet\n",
    "            df = dera_df['num']\n",
    "            num_fields = df.columns\n",
    "            df = df[df.tag=='ComputedTreasuryFloat']\n",
    "            treasury_df = dera_df['sub'].loc[dera_df['sub'].fp=='FY', ['adsh', 'cik', 'name','fye', 'fy', ]].merge(df, on='adsh')\n",
    "            if (len(treasury_df)>0):\n",
    "                display(treasury_df)\n",
    "                grouped_df = treasury_df.groupby('cik')\n",
    "                for key, item in grouped_df:\n",
    "                    if len(item)==1:\n",
    "                        df = item[num_fields].copy()\n",
    "                        df.ddate = pd.to_datetime(f\"{item.fy.squeeze().year}1231\", format='%Y%m%d', utc=True)\n",
    "                        df.version = item.adsh.squeeze()\n",
    "                        # df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        print(\"adding fact (1)\")\n",
    "                        print(df)\n",
    "                        df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        dera_df['num'] = pd.concat([dera_df['num'], df])\n",
    "                    else:\n",
    "                        item = item.sort_values('ddate', ascending=False).reset_index()\n",
    "                        df = generate_intermediate_ddate_value(item.iloc[0:2])\n",
    "                        dera_df['num'] = pd.concat([dera_df['num'], df])\n",
    "            for tbl in dera_tablenames:\n",
    "                if tbl=='tag':\n",
    "                    # Only copy out the new tags in this zipfile.  Older data is already stored\n",
    "                    df = dera_tag[dera_tag.srcdir.eq(fy_qtr)]\n",
    "                    print(f\"len(dera_tag) = {len(dera_tag)}\")\n",
    "                    print(f\"len(df) = {len(df)}\")\n",
    "                else:\n",
    "                    df = dera_df[tbl]\n",
    "                # append_to_trino_table_with_dbt_metadata will prepend dera_table_prefix\n",
    "                append_to_trino_table_with_dbt_metadata(f\"{tbl}\", df, partition_columns=['srcdir'], custom_meta_content='', custom_meta_fields='', verbose=False)\n",
    "        else:\n",
    "            # Import into Trino the slow way...\n",
    "            for tbl in dera_tablenames:\n",
    "                ingest_table = f\"{dera_table_prefix}{tbl}\"\n",
    "                columnschema = osc.create_table_schema_pairs(dera_df[tbl],\n",
    "                                                             typemap={\"int16\":\"integer\", \"Int16\":\"integer\",\n",
    "                                                                      \"datetime64[ns, UTC]\":\"timestamp(6)\"})\n",
    "                tabledef = f\"\"\"\n",
    "create table if not exists {ingest_table}(\n",
    "{columnschema}\n",
    ") with (\n",
    "    partitioning = array['srcdir'],\n",
    "    format = 'ORC'\n",
    ")\n",
    "\"\"\"\n",
    "                print(tabledef)\n",
    "                qres = osc._do_sql(tabledef, engine, verbose=False)\n",
    "                if tbl=='tag':\n",
    "                    # Only copy out the new tags in this zipfile.  Older data is already stored\n",
    "                    df = dera_tag[dera_tag.srcdir.eq(fy_qtr)]\n",
    "                else:\n",
    "                    df = dera_df[tbl]\n",
    "                df.to_sql(ingest_table,\n",
    "                          con=engine, schema=ingest_schema, if_exists='append',\n",
    "                          index=False,\n",
    "                          method=osc.TrinoBatchInsert(batch_size=batch_size[tbl], verbose = False))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35a95b38-b5a7-4b6f-a454-01ac62e622a4",
   "metadata": {},
   "source": [
    "# Once we have all our parquet files in place, load up the tables with their directory contents\n",
    "for tbl in dera_tablenames:\n",
    "    if tbl not in dera_df:\n",
    "        error(f'{tbl} data not found')\n",
    "    tabledef = osc.unmanaged_parquet_tabledef(dera_df[tbl],\n",
    "                                              ingest_catalog, ingest_schema, tbl, trino_bucket,\n",
    "                                              typemap={'int16':'smallint', 'Int16':'smallint'})\n",
    "    qres = cxn.execute(tabledef)\n",
    "    if qres.returns_rows:\n",
    "        for row in qres.fetchall():\n",
    "            print(row)\n",
    "\n",
    "    dataset_query = (f'SELECT * FROM {ingest_catalog}.{ingest_schema}.{dera_table_prefix}{tbl} limit 10')\n",
    "    print(dataset_query)\n",
    "    dataset = cxn.execute(dataset_query)\n",
    "    if dataset.returns_rows:\n",
    "        for row in dataset.fetchall():\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "772604b8-7ba1-475b-8257-831c523c65e0",
   "metadata": {},
   "source": [
    "zipfile_obj = zipfile.ZipFile(f'/tmp/2021q2.zip', mode='r')\n",
    "fy_qtr = tmpname.split('.')[0]\n",
    "for zipinfo in zipfile_obj.infolist():\n",
    "    fname = zipinfo.filename\n",
    "    if fname != 'num.txt':\n",
    "        continue\n",
    "    if fname[3:] != '.txt':\n",
    "        continue\n",
    "    tbl = fname[:3]\n",
    "    if tbl not in dera_tablenames:\n",
    "        continue\n",
    "    ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "    print(f'{fy_qtr} - {tbl}')\n",
    "    with zipfile_obj.open(fname) as zf:\n",
    "        # This fills a directory with parquet files\n",
    "        df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "        df_shares = df[df.tag.isin(share_tags)]\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        print(len(df_shares))\n",
    "        print(len(df_prices))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "714094ff-06b1-4192-8a2e-664c354d70a5",
   "metadata": {},
   "source": [
    "df.loc[df.value=='', 'value'] = pd.NA\n",
    "df[(df.adsh=='0001193125-21-195161')&(df.tag=='SharesOutstanding')].sort_values('value',ascending=False).iloc[0]['value'] is pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375a547-4e34-484b-8ff4-b40416be315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrow metadata code from DERA-iceberg if/when we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285fe0e-e7ce-4c5b-8f2d-d6496e766ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qres = osc._do_sql(f\"select count(*) from {iceberg_catalog}.{iceberg_schema}.ticker_source\", engine)\n",
    "print(\"ticker_source count(*) = \" + str(qres[0][0]))\n",
    "for tbl in dera_tablenames:\n",
    "    qres = osc._do_sql(f\"select count (*),srcdir from {iceberg_catalog}.{iceberg_schema}.{dera_table_prefix}{tbl}_source group by srcdir order by srcdir\", engine)\n",
    "    print(f\"tbl = {tbl}\")\n",
    "    for row in qres:\n",
    "        print(f\"{dera_table_prefix}{tbl}[{row[1]}] count(*) = {row[0]}\")\n",
    "    print(\"\")\n",
    "# print(list(zip(tablenames, l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c473678-b1b8-437f-8ee4-49a4ffbf45b8",
   "metadata": {},
   "source": [
    "Right now we don't really populate data with metadata using code.  We copy in dera_base_schema.yml\n",
    "that has all the metadata (and data quality checks) from the SEC DERA data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbfabd-9dbc-4873-bd89-cc405a436283",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_yml = open(models_dir.joinpath(\"dera_schema.yml\"), \"w\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a35c7-85ad-45fe-9b68-3c371a51a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"version: 2\", file=dbt_yml)\n",
    "\n",
    "indent = 0\n",
    "print(\"\\nmodels:\", file=dbt_yml)\n",
    "indent = indent + 2\n",
    "for name in dbt_dict['models']:\n",
    "    model = dbt_dict['models'][name]\n",
    "    print(f\"{' '*indent}- name: {name}\", file=dbt_yml)\n",
    "    indent = indent + 2\n",
    "    print(f\"{' '*indent}description: {model['description']}\", file=dbt_yml)\n",
    "    print(f\"\\n{' '*indent}columns:\", file=dbt_yml)\n",
    "    indent = indent + 2\n",
    "    columns = model['columns']\n",
    "    for col in columns:\n",
    "        print(f\"{' '*indent}- name: {col}\", file=dbt_yml)\n",
    "        indent = indent + 2\n",
    "        for col_meta in columns[col].keys():\n",
    "            print(f\"{' '*indent}{col_meta}: {columns[col][col_meta]}\", file=dbt_yml)\n",
    "        indent = indent - 2\n",
    "    print(\"\", file=dbt_yml) # newline comes for free...\n",
    "    indent = indent - 4\n",
    "indent = indent - 2\n",
    "assert(indent==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dcba2-a8b5-46c6-8d2f-1391f57794dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_yml.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9b9875a-c2f6-4d77-bd4b-166e00fdba4b",
   "metadata": {},
   "source": [
    "bad_tables = ['ingest_temp_ae4083cb']\n",
    "\n",
    "import osc_ingest_trino.unmanaged as oscu\n",
    "\n",
    "for hive_table in bad_tables:\n",
    "    oscu.drop_unmanaged_table(hive_catalog, hive_schema, hive_table, engine, hive_bucket, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdd22c-021e-41d1-b3c8-6e8ef01d01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cdc78-8b1b-4724-a060-df3ca0f48932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
