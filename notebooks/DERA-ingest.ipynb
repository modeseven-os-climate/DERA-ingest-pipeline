{
 "cells": [
  {
   "cell_type": "raw",
   "id": "629351e6-27cb-463f-aacd-66c0fa3571de",
   "metadata": {},
   "source": [
    "# 001: Share, Price, Revenue -> 314 Market Cap\n",
    "# 002: Revenue, Debt, Float -> 10101.99 Market Cap\n",
    "# 003: Revenue, Price, Share -> 271.8 Market Cap\n",
    "# 004: Revenue, Float, Float2 -> 3141.59 Market Cap\n",
    "\n",
    "df = pd.DataFrame({'adsh' : ['001', '002', '001', '002',\n",
    "                          '001', '002', '003', '003', '003', '004', '004', '004', '005'],\n",
    "                   'sic' : ['4911', '4911', '4911', '4911',\n",
    "                          '4911', '4911', '4911', '4911', '4911', '3210', '3210', '3210', '666'],\n",
    "                   'tag' : ['Shares', 'Revenue', 'Price', 'Debt', 'Revenue', 'Float', 'Revenue', 'Price', 'Shares', 'Revenue', 'Float', 'Float2','Nothing'],\n",
    "                   'ddate' : ['01-01-01', '02-02-02', '01-01-01', '02-02-02', '01-01-01', '02-02-02', '03-03-03', '03-03-03', '03-03-03', '04-04-04', '04-04-04', '04-04-04','05-05-05'],\n",
    "                   'value' : [100, 11.0, 3.14, 2.2, 0.02, 10101.99, 189, 100, 2.718, 1024.48, 2718.28, 3141.59,-1.0]})\n",
    "display(df[df['tag'].isin(['Float'])])\n",
    "\n",
    "# adsh_001[adsh_001['tag']=='Shares']['value'].reset_index() * adsh_001[adsh_001['tag']=='Price']['value'].reset_index()\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        print(key)\n",
    "        df = grouped_df.get_group(key)\n",
    "        df_max = df[df.tag.isin(['Float','Float2'])]\n",
    "        if df_max.empty:\n",
    "            df_max = df[df.tag=='Shares']\n",
    "            if not df_max.empty and not df[df.tag=='Price'].empty:\n",
    "                df_max = df_max.copy()\n",
    "                df_max['value'] = df[df.tag=='Price']['value'].mean() * df_max['value'].squeeze()\n",
    "                df_max['tag'] = 'InferredFloat'\n",
    "        else:\n",
    "            df_max = df_max.sort_values('value', ascending=False).iloc[0].copy()\n",
    "            df_max['tag'] = 'SelectedFloat'\n",
    "        new_df = new_df.append(df_max)\n",
    "    return new_df\n",
    "\n",
    "x = df.groupby(['adsh', 'ddate']).pipe(infer_float)\n",
    "df = df.append(x)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf0869-f139-411a-9b34-51945c12cade",
   "metadata": {},
   "source": [
    "# Ingest SEC DERA data into Trino pipeline\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf87f-0bd3-41e1-acb8-c2508110c510",
   "metadata": {},
   "source": [
    "Run these in a notebook cell if you need to install onto your nb env\n",
    "\n",
    "```python\n",
    "# 'capture' magic prevents long outputs from spamming your notebook\n",
    "%%capture pipoutput\n",
    "\n",
    "# For loading predefined environment variables from files\n",
    "# Typically used to load sensitive access credentials\n",
    "%pip install python-dotenv\n",
    "\n",
    "# Standard python package for interacting with S3 buckets\n",
    "%pip install boto3\n",
    "\n",
    "# Interacting with Trino and using Trino with sqlalchemy\n",
    "%pip install trino sqlalchemy sqlalchemy-trino\n",
    "\n",
    "# Pandas and parquet file i/o\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# OS-Climate utilities to make data ingest easier\n",
    "%pip install osc-ingest-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789567ff-268c-45e8-8f6f-e768ba62f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "from osc_ingest_trino import *\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7692120-2f4f-48be-9847-0f78a3359bc1",
   "metadata": {},
   "source": [
    "Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba14b4-0746-4ce6-9c0f-ff680221bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff328b-df7c-478c-8a47-a3cd7f03803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "env_var_prefix = 'TRINO'\n",
    "\n",
    "sqlstring = 'trino://{user}@{host}:{port}/'.format(\n",
    "    user = os.environ[f'{env_var_prefix}_USER'],\n",
    "    host = os.environ[f'{env_var_prefix}_HOST'],\n",
    "    port = os.environ[f'{env_var_prefix}_PORT']\n",
    ")\n",
    "sqlargs = {\n",
    "    'auth': trino.auth.JWTAuthentication(os.environ[f'{env_var_prefix}_PASSWD']),\n",
    "    'http_scheme': 'https',\n",
    "    'catalog': 'osc_datacommons_dev'\n",
    "}\n",
    "engine = create_engine(sqlstring, connect_args = sqlargs)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e3f55-c8d5-489e-bb5e-9799ff9f68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osc_ingest_trino import *\n",
    "\n",
    "trino_bucket = attach_s3_bucket(\"S3_DEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fa84b-d4c0-4dd6-99da-2cab1641ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "source_bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efe080-b4eb-4713-8fd0-7f1d2e1fd966",
   "metadata": {},
   "source": [
    "Open a Trino connection using JWT for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0dce7-1762-4605-8617-f2bf6d649f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_catalog = 'osc_datacommons_dev'\n",
    "ingest_schema = 'sec_dera'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96687d-74a7-46d8-9944-b63fddf11bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available schemas to ensure trino connection is set correctly\n",
    "schema_read = engine.execute(f'show schemas in {ingest_catalog}')\n",
    "for row in schema_read.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d59fd-f47a-4244-9b4a-3cd6d2d1f164",
   "metadata": {},
   "source": [
    "Enter the Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c5cf5-b15e-4ce0-ad90-e14661950f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b412-e27d-4ccf-a65f-0f00b47affe4",
   "metadata": {},
   "source": [
    "Drop previous tables and schema to start with a fresh slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fde8f5-23b4-4d70-a5bd-0f11abf23b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"show tables in {ingest_schema}\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07150db9-52d1-40df-982c-30d48fb3449c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for view in [ 'assets_by_adsh_ddate', 'assets_usd_by_adsh_ddate', 'assets_xyz_by_adsh_ddate',\n",
    "               'assets_by_lei', 'assets_usd_by_lei', 'assets_xyz_by_lei',\n",
    "               'cash_by_adsh_ddate', 'cash_by_lei', 'cash_usd_by_lei', 'cash_xyz_by_lei',\n",
    "               'debt_by_adsh_ddate', 'debt_by_lei', 'debt_usd_by_lei', 'debt_xyz_by_lei',\n",
    "               'financials_by_lei',\n",
    "               'float_by_adsh_ddate', 'float_by_lei', 'float_usd_by_lei', 'float_xyz_by_lei',\n",
    "               'fy_revenue_by_lei', 'fy_revenue_usd_by_lei', 'fy_revenue_xyz_by_lei',\n",
    "               'fy_income_by_lei', 'fy_income_usd_by_lei', 'fy_income_xyz_by_lei',\n",
    "            ]:\n",
    "    sql = f\"\"\"\n",
    "drop view if exists {ingest_catalog}.{ingest_schema}.{view}\n",
    "\"\"\"\n",
    "    # print(sql)\n",
    "    qres = engine.execute(sql)\n",
    "    # print(qres.fetchall())\n",
    "\n",
    "for ingest_table in [ 'sub', 'num', 'tag', 'ticker', 'average_fx', 'closing_fx', 'sic_isic',\n",
    "                      't_a', 't_c', 't_d', 't_f', 't_r', 't_i']:\n",
    "    sql = f\"\"\"\n",
    "drop table if exists {ingest_catalog}.{ingest_schema}.{ingest_table}\n",
    "\"\"\"\n",
    "    # print(sql)\n",
    "    qres = engine.execute(sql)\n",
    "    # print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ce42d-35de-42ce-8aae-c1e3228c7d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = f\"show tables in {ingest_schema}\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())\n",
    "\n",
    "sql = f\"\"\"\n",
    "drop schema if exists {ingest_catalog}.{ingest_schema}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdb125-1769-40f8-ace7-6daa29456c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure schema exists, or table creation below will fail in weird ways\n",
    "sql = f\"\"\"\n",
    "create schema {ingest_catalog}.{ingest_schema}\n",
    "\"\"\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77a01d-6558-4da9-a6af-5ba01e196b3b",
   "metadata": {},
   "source": [
    "For now, create SIC -> ISIC crosswalk by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412410ce-c37d-46b8-b278-c286f6543c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sic_isic = {\n",
    "    2911: 1920, # Petroleum refining\n",
    "    3714: 2910, # Motor Vehicle Manufacturing\n",
    "    3312: 2410, # Steel\n",
    "    4911: 4010, # Electricity Generation\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(sic_isic, orient='index', columns=['isic'])\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index':'sic'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1a2fd-b35c-40e3-9e9c-177bf2a437a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "df.to_parquet(path=buf)\n",
    "buf.seek(0)\n",
    "trino_bucket.upload_fileobj(Fileobj=buf,\n",
    "                      Key=f'trino/{ingest_schema}/sic_isic/data.parquet')\n",
    "\n",
    "sql = f\"\"\"\n",
    "drop table if exists {ingest_schema}.sic_isic;\n",
    "create table {ingest_schema}.sic_isic(\n",
    "    sic integer,\n",
    "    isic integer\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{trino_bucket.name}/trino/{ingest_schema}/sic_isic/'\n",
    ")\n",
    "\"\"\"\n",
    "for sql_stmt in sql.split(';'):\n",
    "    print(sql_stmt)\n",
    "    qres = engine.execute(sql_stmt)\n",
    "    print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b20942-417b-40c7-a6b0-3bda008b1d98",
   "metadata": {},
   "source": [
    "Load `ticker` file (updated sporadically from https://www.sec.gov/include/ticker.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5495730-2363-42af-b146-7eca5cdb201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'SEC-DERA/ticker.txt')\n",
    "ticker_file.download_file(f'/tmp/dera-ticker.txt')\n",
    "ticker_df = pd.read_csv(f'/tmp/dera-ticker.txt', names=['tname', 'cik'], header=None, sep='\\t', dtype={'tname':'string','cik':'int64'}, engine='c')\n",
    "ticker_dict = dict(zip(ticker_df.cik, ticker_df.tname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade336d-a3f7-4edf-989e-f9e8c0bad982",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f886392-62d5-4f94-abc7-82a5f0d9e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "ticker_df.to_parquet(path=buf)\n",
    "buf.seek(0)\n",
    "trino_bucket.upload_fileobj(Fileobj=buf,\n",
    "                      Key=f'trino/{ingest_schema}/ticker/data.parquet')\n",
    "\n",
    "sql = f\"\"\"\n",
    "drop table if exists {ingest_catalog}.{ingest_schema}.ticker;\n",
    "create table {ingest_catalog}.{ingest_schema}.ticker(\n",
    "    cik bigint,\n",
    "    tname varchar\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{trino_bucket.name}/trino/{ingest_schema}/ticker/'\n",
    ");\n",
    "select count (*) from {ingest_catalog}.{ingest_schema}.ticker;\n",
    "select * from {ingest_catalog}.{ingest_schema}.ticker limit 10\n",
    "\"\"\"\n",
    "for sql_stmt in sql.split(';'):\n",
    "    print(sql_stmt)\n",
    "    qres = engine.execute(sql_stmt)\n",
    "    print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f3bf-da3f-4d85-94ea-0783bebf9ed2",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28874f-b565-4231-927f-f0db6b9364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleif_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'mtiemann-GLEIF/DERA-matches.csv')\n",
    "gleif_file.download_file(f'/tmp/dera-gleif.csv')\n",
    "gleif_df = pd.read_csv(f'/tmp/dera-gleif.csv', header=0, sep=',', dtype=str, engine='c')\n",
    "gleif_dict = dict(zip(gleif_df.name, gleif_df.LEI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902df315-a8f2-43e3-9d1c-f1bdddeae1a9",
   "metadata": {},
   "source": [
    "Load the SUB, NUM, and TAG tables into Trino"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5959517e-cf61-4c01-b236-908726fec3ff",
   "metadata": {},
   "source": [
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "rmi_20211120_b = io.BytesIO(source_bucket.Object('RMI/RMI-20211120.zip').get()['Body'].read())\n",
    "rmi_20211120_zip = zipfile.ZipFile(rmi_20211120_b, mode='r')\n",
    "del(rmi_20211120_b)\n",
    "# display(zipfile.ZipFile(rmi_20211120_zip, mode='r').filelist)\n",
    "rmi_dd = rmi_20211120_zip.read('data_dictionary.xlsx')\n",
    "\n",
    "# Read all the sheets.  rmi_excel['sheet_name'] gives a specific sheet\n",
    "rmi_20211120_xls = pd.read_excel(rmi_dd, sheet_name=None, dtype=str)\n",
    "\n",
    "del(rmi_dd)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cce65b5a-6fec-47a1-8646-0faffd316513",
   "metadata": {},
   "source": [
    "from io import BytesIO\n",
    "import datetime\n",
    "\n",
    "# The dataframe for Non-Null data\n",
    "df_nn = None\n",
    "\n",
    "rmi_ingest_schemas = { 'rmi_20210929': ('September 2021', rmi_20210929_xls, rmi_20210929_zip),\n",
    "                       'rmi_20211120': ('November 2021', rmi_20211120_xls, rmi_20211120_zip) }\n",
    "\n",
    "# There is no datafile behind the data dictionary.  Run this to prime overview_dd, which all other metadata-finding depends upon\n",
    "for schemaname, (release_date, workbook, zipfile) in rmi_ingest_schemas.items():\n",
    "    overview_meta_fields, overview_meta_content = generate_sheet_meta(workbook, 'Overview', release_date)\n",
    "\n",
    "    for zipinfo in zipfile.infolist():\n",
    "        fname = zipinfo.filename\n",
    "        ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "        if fname[-4:] != '.csv':\n",
    "            continue\n",
    "        \n",
    "        tablename = fname.split('/')[-1].split('.')[0]\n",
    "        # For as-yet unexplained reasons, RMI renamed the file assets_earnings_investments.csv\n",
    "        # without updating the name of the sheet in the spreadsheet.  We switch to the intended name here\n",
    "        if tablename == 'assets_earnings':\n",
    "            tablename = 'assets_earnings_investments'\n",
    "\n",
    "        print(fname)\n",
    "        ## EXAMPLES OF TABLE-SPECIFIC READING FOLLOWS\n",
    "        with zipfile.open(fname) as zf:\n",
    "            if tablename=='state_utility_policies':\n",
    "                df = pd.read_csv(zf, dtype={'respondent_id':'int32'},parse_dates=['date_updated'],dayfirst=True, engine='c')\n",
    "            elif tablename.startswith('utility'):\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], engine='c')\n",
    "            elif tablename=='state_targets':\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], engine='c')\n",
    "                df.year.fillna('-1', inplace=True)\n",
    "                df.loc[df.year.isin(['Annual','2005/1990']), 'year'] = '-1'\n",
    "                df.year = pd.to_datetime(df.year.map(lambda x: x.split('.')[0]).astype('int32'), format='%Y', errors='coerce')\n",
    "            else:\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], thousands=',', engine='c')\n",
    "                if 'year' in df.columns:\n",
    "                    df.year.fillna('-1', inplace=True)\n",
    "                    df.year = pd.to_datetime(df.year.map(lambda x: x.split('.')[0]).astype('int32'), format='%Y', errors='coerce')\n",
    "                df.info(verbose=True)\n",
    "        ## TABLE-SPECIFIC PROCESSING REPLACES THIS COMMENT\n",
    "        create_trino_pipeline (s3_trino, schemaname, tablename, ftimestamp, df, custom_meta_fields, custom_meta_content)\n",
    "    zipfile.close()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e283825-6ea7-40cc-93c7-1ce41084300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "# Borrowed/stole this definition from SEC Corp Financials notebook...\n",
    "float_tags = [\n",
    "    'EntityPublicFloat',\n",
    "    'FreeFloat',\n",
    "    'PublicFloat',\n",
    "    'PublicFloatValue',\n",
    "]\n",
    "\n",
    "# These are in priority preference order\n",
    "share_tags = [\n",
    "    'EntityCommonStockSharesOutstanding',\n",
    "    'CommonStockSharesOutstanding',\n",
    "    'SharesOutstanding',\n",
    "    'WeightedAverageNumberOfDilutedSharesOutstanding',\n",
    "    'WeightedAverageNumberOfSharesOutstandingBasic',\n",
    "]\n",
    "\n",
    "shareprice_tags = [\n",
    "    'SharePrice',\n",
    "    'PerSharePrice',\n",
    "    'CommonStockValueOne',\n",
    "    'MarketValuePerShare',\n",
    "    'SaleOfStockPricePerShare',\n",
    "    'CashPricePerOrdinaryShare',\n",
    "    'TreasurySharesValuePerShare',\n",
    "    'SharesOutstandingPricePerShare',\n",
    "]\n",
    "\n",
    "treasury_share_tags = [\n",
    "    'TreasuryStockShares',\n",
    "    # 'TreasuryStockShares1',\n",
    "]\n",
    "\n",
    "treasury_value_tags = [\n",
    "    'TreasurySharesMarketValue',\n",
    "    'FairValueOfTreasuryShares',\n",
    "    'MarketValueOfTreasuryShares',\n",
    "]\n",
    "\n",
    "ambiguous_tags = [\n",
    "    # For AES (and about 10% of others), CommonStockValue is a share price\n",
    "    # For Excelon (and about 90% of others), it's the total value of outstanding common shares\n",
    "    'CommonStockValue',\n",
    "    'CommonStockValueOutstanding',\n",
    "]\n",
    "\n",
    "all_float_helper_tags = share_tags + shareprice_tags + treasury_share_tags + treasury_value_tags + ambiguous_tags\n",
    "\n",
    "dera_regex = re.compile(r' ?/.*$')\n",
    "dera_df = {}\n",
    "\n",
    "from math import floor\n",
    "\n",
    "def generate_intermediate_ddate_value(df_price, df_shares):\n",
    "    if len(df_shares)==1:\n",
    "        df = df_shares.iloc[[0]].copy()\n",
    "        return df\n",
    "    year1 = df_shares.iloc[0].ddate\n",
    "    year2 = df_shares.iloc[1].ddate\n",
    "    if (year1-year2).days > 731:\n",
    "        print(\"gap years\")\n",
    "        print(df_shares.iloc[0:2])\n",
    "    df = df_shares.iloc[[0]].copy()\n",
    "    year_end = pd.to_datetime(f\"{floor((year1.year+year2.year)/2.0)}1231\", format='%Y%m%d', utc=True)\n",
    "    df.ddate = year_end\n",
    "    if year1.year==year2.year:\n",
    "        print(\"same years\")\n",
    "        print(df_shares.iloc[0:2])\n",
    "        df.value = (df_shares.iloc[0].value + df_shares.iloc[1].value)/2.0\n",
    "    else:\n",
    "        df.value = ((365.0-(year1-year_end).days)*df.value + (year_end-year2).days*df_shares.iloc[1].value)/365.0\n",
    "    df.version = df_shares.iloc[0].adsh\n",
    "    print(f\"adding fact ({df.tag})\")\n",
    "    display(df)\n",
    "    return df\n",
    "\n",
    "# When this function is called, we already know that we have no matches in FLOAT_TAGS.\n",
    "# GROUPED_DF is grouped by ADSH and only for annual reports.  DDATE can be anything (because many reports look back 1-5 years)\n",
    "# We are working these annual reports quarter by quarter for the quarter in which they are reported\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        df = grouped_df.get_group(key)\n",
    "        # We have no overall float value.  Build from shares outstanding * price\n",
    "        df_shares = df[df.tag.isin(share_tags) & (df.value>0)]\n",
    "        if df_shares.empty:\n",
    "            # if not df[df.tag.isin(ambiguous_tags)].empty:\n",
    "            #     print(f\"CommonStockValue exists w/o Shares: {df.head()}\\n\")\n",
    "            #     pass\n",
    "            continue\n",
    "        else:\n",
    "            for share_tag in share_tags:\n",
    "                if not df_shares[df_shares.tag==share_tag].empty:\n",
    "                    df_shares = df_shares[df_shares.tag==share_tag]\n",
    "                    break\n",
    "            df_shares = df_shares.sort_values('ddate', ascending=False)\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        # We have no overall price.  Build from price derived from treasury valuation\n",
    "        if df_prices.empty:\n",
    "            df_treasury_shares = df[df.tag.isin(treasury_share_tags)]\n",
    "            df_treasury_value = df[df.tag.isin(treasury_value_tags)]\n",
    "            if df_treasury_shares.empty or df_treasury_value.empty:\n",
    "                # if not df[df.tag.isin(ambiguous_tags)].empty:\n",
    "                #     print(f\"CommonStockValue exists w/o Treasury Shares+Prices: {df.head()}\\n\")\n",
    "                #     pass\n",
    "                continue\n",
    "            df_svp = df_treasury_value.merge(df_treasury_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                print(f\"{df.adsh.iat[0]}: merge failed (1)\")\n",
    "                continue\n",
    "            # Pick latest date / largest number of shares as basis\n",
    "            df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]].copy()\n",
    "            price_per_share = df_float.value_x.squeeze() / df_float.value_y.squeeze()\n",
    "            df_float.rename(columns={'uom_x', 'uom'},inplace=True)\n",
    "            tag = 'ComputedTreasuryFloat'\n",
    "        else:\n",
    "            # if df_prices[df_prices.tag.str.startswith('ShareBasedCompensationArrangementByShareBasedPaymentAward')].empty:\n",
    "            #     print(f\"Must use market prices; len(df_prices) =  {len(df_prices)}\")\n",
    "            # else:\n",
    "            #     print(f\"Can use Share Based Comp {df_prices.tag.str[45:]}:\\n{df_prices}\\n\\n\")\n",
    "            # We derive a price from market reports\n",
    "            df_svp = df_prices.merge(df_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                if len(df_prices)==1:\n",
    "                    df_shares = generate_intermediate_ddate_value(df_prices, df_shares.sort_values('ddate', ascending=False))\n",
    "                    df_float = df_shares\n",
    "                    price_per_share = df_prices.value.squeeze()\n",
    "                    # print(\"merge rescued (2)\")\n",
    "                    # display(df_shares)\n",
    "                else:\n",
    "                    print(f\"{df.adsh.iat[0]}: merge failed (2)\")\n",
    "                    print(f\"len(df_prices) = {len(df_prices)}\")\n",
    "                    print(f\"len(df_shares) = {len(df_shares)}\")\n",
    "                    display(df_prices)\n",
    "                    display(df_shares)\n",
    "                    continue\n",
    "            else:\n",
    "                # Pick latest date / largest number of shares as basis\n",
    "                df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]].copy()\n",
    "                price_per_share = df_float.value_x.squeeze() # value_x is a price in this case\n",
    "                df_float.rename(columns={'uom_x':'uom'},inplace=True)\n",
    "            tag = 'ComputedMarketFloat'\n",
    "        df_float = df_float[['adsh', 'ddate', 'uom', 'coreg']].copy()\n",
    "        df_float['tag'] = tag\n",
    "        # TODO: should connect price ddate with total shares ddate\n",
    "        total_shares = df_shares.iloc[0].value\n",
    "        df_float['value'] = price_per_share * total_shares\n",
    "        df_float['qtrs'] = 0\n",
    "        df_float['srcdir'] = 'computed'\n",
    "        df_float['version'] = df_float['footnote'] = pd.NA\n",
    "        df_float = df_float.astype(df.drop(columns=['fp']).dtypes.to_dict())\n",
    "        new_df = new_df.append(df_float)\n",
    "    return new_df\n",
    "\n",
    "def read_dera_table(zf, fy_qtr, tbl):\n",
    "    \"\"\"From a local file ZF, read data for the period FY_QTR for the DERA table TBL.\n",
    "    Return the Dataframe created so that when it is time to create the actual Trino table\n",
    "    we know what the shape of the data should look like.  The returned DF has all the data\n",
    "    of the specific ingestion, not all the data of all the ingestions of data for TBL.\"\"\"\n",
    "    global dera_df\n",
    "    \n",
    "    df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "    df['srcdir'] = fy_qtr\n",
    "    df.srcdir = df.srcdir.astype('string')\n",
    "    \n",
    "    # df = df.convert_dtypes (infer_objects=False, convert_string=True, convert_integer=False, convert_boolean=False, convert_floating=False)\n",
    "    # Print the output\n",
    "    # print(df.dtypes)\n",
    "    \n",
    "    if tbl=='sub':\n",
    "        df.name = df.name.map(lambda x: re.sub(dera_regex, '', x))\n",
    "        df.name = df.name.astype('string')\n",
    "        df['LEI'] = df.name.map(gleif_dict)\n",
    "        df.LEI = df.LEI.astype('string')\n",
    "        df.cik = df.cik.astype('int32')\n",
    "        df.loc[df.sic=='', 'sic'] = pd.NA\n",
    "        df.sic = df.sic.astype('Int16')\n",
    "        df.loc[df.ein=='', 'ein'] = pd.NA\n",
    "        df.ein = df.ein.astype('Int64')\n",
    "        df.wksi = df.wksi.astype('bool')\n",
    "        # df.wksi = df.wksi.astype('int32')\n",
    "        df.period = pd.to_datetime(df.period, format='%Y%m%d', utc=True, errors='coerce')\n",
    "        df.fy = pd.to_datetime(df.fy, format='%Y', utc=True, errors='coerce')\n",
    "        df.filed = pd.to_datetime(df.filed, format='%Y%m%d', utc=True)\n",
    "        df.accepted = pd.to_datetime(df.accepted, format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "        df.prevrpt = df.prevrpt.astype('bool')\n",
    "        df.detail = df.detail.astype('bool')\n",
    "        df.nciks = df.nciks.astype('int16')\n",
    "        \n",
    "        cols = df.columns.tolist()\n",
    "        # Move LEI to a more friendly location in the column order\n",
    "        cols = cols[0:3] + [cols[-1]] + cols[3:-1]\n",
    "        df = df[cols]\n",
    "    elif tbl=='num':\n",
    "        # documentation wrongly lists coreg as NUMERIC length 256.  It is ALPHANUMERIC.\n",
    "        if fy_qtr=='2021q3':\n",
    "            df.loc[df.ddate=='30210630', 'ddate'] = '20210630'\n",
    "        if fy_qtr=='2019q2':\n",
    "            df.loc[df.ddate=='29171231', 'ddate'] = '20171231'\n",
    "        # Fix some bad AES data\n",
    "        if fy_qtr=='2021q1':\n",
    "            df.loc[(df.adsh=='0000874761-21-000015')&(df.tag=='CommonStockValue')&(df.ddate=='20190630'), 'ddate'] = '20200630'\n",
    "        elif fy_qtr=='2020q1':\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='EntityPublicFloat')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='CommonStockValue')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "        df.ddate = pd.to_datetime(df.ddate, format='%Y%m%d', utc=True)\n",
    "        df.qtrs = df.qtrs.astype('int16')\n",
    "        df.loc[df.coreg=='', 'coreg'] = pd.NA\n",
    "        df.loc[df.value=='', 'value'] = pd.NA\n",
    "        df.value = df.value.astype('Float64')\n",
    "        df.loc[df.footnote=='', 'footnote'] = pd.NA\n",
    "        \n",
    "        print(f\"Inferring floats: start len(df) = {len(df)}\")\n",
    "        annual_df = dera_df['sub'][dera_df['sub'].form.isin(['10-K','20-F','40-F'])]\n",
    "        df['fp'] = df.adsh.map(dict(zip(annual_df.adsh,annual_df.fp)))\n",
    "        print(f\"len(df[df.fp=='FY']) = {len(df[df.fp=='FY'])}\")\n",
    "        df = df[df.fp=='FY']\n",
    "        # df = df.assign(cik=df.adsh.str[:10])\n",
    "        df_has_float = df[df.tag.isin(float_tags)]\n",
    "        print(f\"len(df_has_float) = {len(df_has_float)}\")\n",
    "        df_needs_float = df[~df.adsh.isin(df_has_float.adsh)]\n",
    "        print(f\"len(df_needs_float) = {len(df_needs_float)}\")\n",
    "        float_df = infer_float(df_needs_float[df_needs_float.coreg.isna()\n",
    "                                              &df_needs_float.tag.isin(all_float_helper_tags)\n",
    "                                              &(df_needs_float.value>0)].groupby(['adsh'], as_index=False))\n",
    "        df = df.drop(columns=['fp'])\n",
    "        float_df = float_df.astype(df.dtypes.to_dict())\n",
    "        print(f\"{len(float_df)} floats inferred; {len(float_df[float_df.tag=='ComputedTreasuryFloat'])} treasury-based; {len(float_df[float_df.tag=='ComputedMarketFloat'])} market-based\")\n",
    "        df = df.append(float_df, ignore_index=True)\n",
    "    elif tbl=='tag':\n",
    "        df.custom = df.custom.astype('bool')\n",
    "        df.abstract = df.abstract.astype('bool')\n",
    "        df.loc[df.crdr=='', 'crdr'] = pd.NA\n",
    "        df.loc[df.tlabel=='', 'tlabel'] = pd.NA\n",
    "        df.loc[df.doc=='', 'doc'] = pd.NA\n",
    "    # print(df.dtypes)\n",
    "    # display(df.head())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ab7b7-4f5c-4957-9616-6df5820856cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "objects=source_bucket.objects.filter(Prefix='SEC-DERA/20')\n",
    "\n",
    "dera_tables = ['sub', 'num', 'tag']\n",
    "\n",
    "for obj in objects:\n",
    "    if obj.key.endswith('.zip'):\n",
    "        zipfile_src = s3_source.Object(os.environ['S3_LANDING_BUCKET'],obj.key)\n",
    "        tmpname = obj.key.split('/')[-1]\n",
    "        zipfile_src.download_file(f'/tmp/{tmpname}')\n",
    "        zipfile_obj = zipfile.ZipFile(f'/tmp/{tmpname}', mode='r')\n",
    "        fy_qtr = tmpname.split('.')[0]\n",
    "        for tbl in dera_tables:\n",
    "            print(f'{fy_qtr} - {tbl}')\n",
    "            with zipfile_obj.open(f\"{tbl}.txt\") as zf:\n",
    "                # Read data from ZF into a dataframe.\n",
    "                dera_df[tbl] = read_dera_table (zf, fy_qtr, tbl)\n",
    "        zipfile_obj.close()\n",
    "\n",
    "        if False:\n",
    "            # Alas, there is some minor post-fixing we need to do before ingesting into parquet\n",
    "            df = dera_df['num']\n",
    "            num_fields = df.columns\n",
    "            df = df[df.tag=='ComputedTreasuryFloat']\n",
    "            treasury_df = dera_df['sub'].loc[dera_df['sub'].fp=='FY', ['adsh', 'cik', 'name','fye', 'fy', ]].merge(df, on='adsh')\n",
    "            if (len(treasury_df)>0):\n",
    "                display(treasury_df)\n",
    "                grouped_df = treasury_df.groupby('cik')\n",
    "                for key, item in grouped_df:\n",
    "                    if len(item)==1:\n",
    "                        df = item[num_fields].copy()\n",
    "                        df.ddate = pd.to_datetime(f\"{item.fy.squeeze().year}1231\", format='%Y%m%d')\n",
    "                        df.version = item.adsh.squeeze()\n",
    "                        # df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        print(\"adding fact (1)\")\n",
    "                        print(df)\n",
    "                        df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        dera_df['num'] = dera_df['num'].append(df)\n",
    "                    else:\n",
    "                        item = item.sort_values('ddate', ascending=False).reset_index()\n",
    "                        df = generate_intermediate_ddate_value(item.iloc[0:2])\n",
    "                        dera_df['num'] = dera_df['num'].append(df)\n",
    "        \n",
    "        # Now output as parquet files\n",
    "        for tbl in dera_tables:\n",
    "            buf = io.BytesIO()\n",
    "            dera_df[tbl].to_parquet(path=buf)\n",
    "            buf.seek(0)\n",
    "            fy_qtr = dera_df[tbl].iloc[0].srcdir\n",
    "            trino_bucket.upload_fileobj(Fileobj=buf,\n",
    "                                        Key=f'trino/{ingest_schema}/{tbl}/{fy_qtr}.parquet')\n",
    "\n",
    "# Once we have all our parquet files in place, load up the tables with their directory contents\n",
    "for tbl in dera_tables:\n",
    "    if tbl not in dera_df:\n",
    "        error(f'{tbl} data not found')\n",
    "    table_check = engine.execute(f'drop table if exists {ingest_catalog}.{ingest_schema}.{tbl}')\n",
    "    for row in table_check.fetchall():\n",
    "        print(row)\n",
    "\n",
    "    columnschema = create_table_schema_pairs(dera_df[tbl], typemap={'int16':'smallint', 'Int16':'smallint'})\n",
    "    tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{tbl} (\n",
    "{columnschema}\n",
    ") with (\n",
    "format = 'parquet',\n",
    "external_location = 's3a://{trino_bucket.name}/trino/{ingest_schema}/{tbl}/'\n",
    ")\n",
    "\"\"\"\n",
    "    print(tabledef)\n",
    "\n",
    "    table_create = engine.execute(tabledef)\n",
    "    for row in table_create.fetchall():\n",
    "        print(row)\n",
    "\n",
    "    dataset_query = (f'SELECT * FROM {ingest_catalog}.{ingest_schema}.{tbl} limit 10')\n",
    "    print(dataset_query)\n",
    "    dataset = engine.execute(dataset_query)\n",
    "    for row in dataset.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "772604b8-7ba1-475b-8257-831c523c65e0",
   "metadata": {},
   "source": [
    "zipfile_obj = zipfile.ZipFile(f'/tmp/2021q2.zip', mode='r')\n",
    "fy_qtr = tmpname.split('.')[0]\n",
    "for zipinfo in zipfile_obj.infolist():\n",
    "    fname = zipinfo.filename\n",
    "    if fname != 'num.txt':\n",
    "        continue\n",
    "    if fname[3:] != '.txt':\n",
    "        continue\n",
    "    tbl = fname[:3]\n",
    "    if tbl not in dera_tables:\n",
    "        continue\n",
    "    ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "    print(f'{fy_qtr} - {tbl}')\n",
    "    with zipfile_obj.open(fname) as zf:\n",
    "        # This fills a directory with parquet files\n",
    "        df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "        df_shares = df[df.tag.isin(share_tags)]\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        print(len(df_shares))\n",
    "        print(len(df_prices))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "714094ff-06b1-4192-8a2e-664c354d70a5",
   "metadata": {},
   "source": [
    "df.loc[df.value=='', 'value'] = pd.NA\n",
    "df[(df.adsh=='0001193125-21-195161')&(df.tag=='SharesOutstanding')].sort_values('value',ascending=False).iloc[0]['value'] is pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375a547-4e34-484b-8ff4-b40416be315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrow metadata code from DERA-iceberg if/when we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285fe0e-e7ce-4c5b-8f2d-d6496e766ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablenames = ['sub', 'num', 'tag', 'ticker']\n",
    "l = []\n",
    "for tbl in tablenames:\n",
    "    qres = engine.execute(f'select count (*) from {ingest_catalog}.{ingest_schema}.{tbl}')\n",
    "    l.append(qres.fetchall()[0])\n",
    "print(list(zip(tablenames, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdda70f-1ee8-44a1-9a47-c3ddcbdbd065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
