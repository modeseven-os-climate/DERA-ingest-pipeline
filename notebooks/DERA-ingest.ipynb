{
 "cells": [
  {
   "cell_type": "raw",
   "id": "629351e6-27cb-463f-aacd-66c0fa3571de",
   "metadata": {},
   "source": [
    "# 001: Share, Price, Revenue -> 314 Market Cap\n",
    "# 002: Revenue, Debt, Float -> 10101.99 Market Cap\n",
    "# 003: Revenue, Price, Share -> 271.8 Market Cap\n",
    "# 004: Revenue, Float, Float2 -> 3141.59 Market Cap\n",
    "\n",
    "df = pd.DataFrame({'adsh' : ['001', '002', '001', '002',\n",
    "                          '001', '002', '003', '003', '003', '004', '004', '004', '005'],\n",
    "                   'sic' : ['4911', '4911', '4911', '4911',\n",
    "                          '4911', '4911', '4911', '4911', '4911', '3210', '3210', '3210', '666'],\n",
    "                   'tag' : ['Shares', 'Revenue', 'Price', 'Debt', 'Revenue', 'Float', 'Revenue', 'Price', 'Shares', 'Revenue', 'Float', 'Float2','Nothing'],\n",
    "                   'ddate' : ['01-01-01', '02-02-02', '01-01-01', '02-02-02', '01-01-01', '02-02-02', '03-03-03', '03-03-03', '03-03-03', '04-04-04', '04-04-04', '04-04-04','05-05-05'],\n",
    "                   'value' : [100, 11.0, 3.14, 2.2, 0.02, 10101.99, 189, 100, 2.718, 1024.48, 2718.28, 3141.59,-1.0]})\n",
    "display(df[df['tag'].isin(['Float'])])\n",
    "\n",
    "# adsh_001[adsh_001['tag']=='Shares']['value'].reset_index() * adsh_001[adsh_001['tag']=='Price']['value'].reset_index()\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        print(key)\n",
    "        df = grouped_df.get_group(key)\n",
    "        df_max = df[df.tag.isin(['Float','Float2'])]\n",
    "        if df_max.empty:\n",
    "            df_max = df[df.tag=='Shares']\n",
    "            if not df_max.empty and not df[df.tag=='Price'].empty:\n",
    "                df_max = df_max.copy()\n",
    "                df_max['value'] = df[df.tag=='Price']['value'].mean() * df_max['value'].squeeze()\n",
    "                df_max['tag'] = 'InferredFloat'\n",
    "        else:\n",
    "            df_max = df_max.sort_values('value', ascending=False).iloc[0].copy()\n",
    "            df_max['tag'] = 'SelectedFloat'\n",
    "        new_df = new_df.append(df_max)\n",
    "    return new_df\n",
    "\n",
    "x = df.groupby(['adsh', 'ddate']).pipe(infer_float)\n",
    "df = df.append(x)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf0869-f139-411a-9b34-51945c12cade",
   "metadata": {},
   "source": [
    "# Ingest SEC DERA data into Trino pipeline\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf87f-0bd3-41e1-acb8-c2508110c510",
   "metadata": {},
   "source": [
    "Run these in a notebook cell if you need to install onto your nb env\n",
    "\n",
    "```python\n",
    "# 'capture' magic prevents long outputs from spamming your notebook\n",
    "%%capture pipoutput\n",
    "\n",
    "# For loading predefined environment variables from files\n",
    "# Typically used to load sensitive access credentials\n",
    "%pip install python-dotenv\n",
    "\n",
    "# Standard python package for interacting with S3 buckets\n",
    "%pip install boto3\n",
    "\n",
    "# Interacting with Trino and using Trino with sqlalchemy\n",
    "%pip install trino sqlalchemy sqlalchemy-trino\n",
    "\n",
    "# Pandas and parquet file i/o\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# OS-Climate utilities to make data ingest easier\n",
    "%pip install osc-ingest-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789567ff-268c-45e8-8f6f-e768ba62f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "import osc_ingest_trino as osc\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7692120-2f4f-48be-9847-0f78a3359bc1",
   "metadata": {},
   "source": [
    "Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ba14b4-0746-4ce6-9c0f-ff680221bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42269f57-27f0-4439-806f-10a1c84ae5fa",
   "metadata": {},
   "source": [
    "Open a Trino connection using JWT for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a0dce7-1762-4605-8617-f2bf6d649f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_catalog = 'osc_datacommons_dev'\n",
    "ingest_schema = 'sandbox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07a1c2b-a2cf-4afe-bbef-041542ac4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "engine = osc.attach_trino_engine(env_var_prefix=\"TRINO\", catalog=ingest_catalog, schema=ingest_schema, verbose=False)\n",
    "\n",
    "# Needed for parquet two-step\n",
    "trino_bucket = osc.attach_s3_bucket(\"S3_DEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84fa84b-d4c0-4dd6-99da-2cab1641ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "source_bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb96687d-74a7-46d8-9944-b63fddf11bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('default',)\n",
      "('demo_dv',)\n",
      "('iceberg_demo',)\n",
      "('information_schema',)\n",
      "('mdt_sandbox',)\n",
      "('pcaf_sovereign_footprint',)\n",
      "('sandbox',)\n"
     ]
    }
   ],
   "source": [
    "# Show available schemas to ensure trino connection is set correctly\n",
    "schema_read = engine.execute(f'show schemas in {ingest_catalog}')\n",
    "for row in schema_read.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d59fd-f47a-4244-9b4a-3cd6d2d1f164",
   "metadata": {},
   "source": [
    "Enter the Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61c5cf5-b15e-4ce0-ad90-e14661950f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b412-e27d-4ccf-a65f-0f00b47affe4",
   "metadata": {},
   "source": [
    "Drop previous tables and schema to start with a fresh slate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b286239a-dedf-466e-9e0c-86f9f4348139",
   "metadata": {},
   "source": [
    "sql = f\"show tables in {ingest_schema}\"\n",
    "print(sql)\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3da21177-1d1e-4aad-801a-9948ee6cf914",
   "metadata": {
    "tags": []
   },
   "source": [
    "for view in [ 'assets_by_lei', 'assets_usd_by_lei', 'assets_xyz_by_lei',\n",
    "              'cash_by_adsh_ddate', 'cash_by_lei', 'cash_usd_by_lei', 'cash_xyz_by_lei',\n",
    "              'debt_by_adsh_ddate', 'debt_by_lei', 'debt_usd_by_lei', 'debt_xyz_by_lei',\n",
    "              'financials_by_lei',\n",
    "              'float_by_lei', 'float_usd_by_lei', 'float_xyz_by_lei',\n",
    "              'fy_revenue_by_lei', 'fy_revenue_usd_by_lei', 'fy_revenue_xyz_by_lei',\n",
    "              'fy_income_by_lei', 'fy_income_usd_by_lei', 'fy_income_xyz_by_lei',\n",
    "            ]:\n",
    "    sql = f\"\"\"\n",
    "drop view if exists {ingest_catalog}.{ingest_schema}.{view}\n",
    "\"\"\"\n",
    "    # print(sql)\n",
    "    qres = engine.execute(sql)\n",
    "    qres.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b20942-417b-40c7-a6b0-3bda008b1d98",
   "metadata": {},
   "source": [
    "Load `ticker` file (updated sporadically from https://www.sec.gov/include/ticker.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94deb145-7dd9-443c-a93d-38b0e82153ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'SEC-DERA/ticker.txt')\n",
    "ticker_file.download_file(f'/tmp/dera-ticker.txt')\n",
    "ticker_df = pd.read_csv(f'/tmp/dera-ticker.txt', names=['tname', 'cik'], header=None, sep='\\t', dtype={'tname':'string','cik':'int64'}, engine='c')\n",
    "ticker_dict = dict(zip(ticker_df.cik, ticker_df.tname))\n",
    "\n",
    "ticker_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "782c6436-6f73-436d-8abe-245d557ecc1d",
   "metadata": {},
   "source": [
    "ingest_table = 'ticker'\n",
    "columnschema = osc.create_table_schema_pairs(ticker_df)\n",
    "\n",
    "qres = engine.execute(f\"drop table if exists {ingest_catalog}.{ingest_schema}.{ingest_table}\")\n",
    "qres.fetchall()\n",
    "\n",
    "tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{ingest_table}(\n",
    "{columnschema}\n",
    ") with (\n",
    "partitioning = array['bucket(tname,10)'],\n",
    "format = 'ORC'\n",
    ")\n",
    "\"\"\"\n",
    "print(tabledef)\n",
    "qres = engine.execute(tabledef)\n",
    "print(qres.fetchall())\n",
    "ticker_df.to_sql(ingest_table,\n",
    "                 con=engine, schema=ingest_schema, if_exists='append',\n",
    "                 index=False,\n",
    "                 method=osc.TrinoBatchInsert(batch_size = 12000, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f3bf-da3f-4d85-94ea-0783bebf9ed2",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28874f-b565-4231-927f-f0db6b9364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleif_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],'mtiemann-GLEIF/DERA-matches.csv')\n",
    "gleif_file.download_file(f'/tmp/dera-gleif.csv')\n",
    "gleif_df = pd.read_csv(f'/tmp/dera-gleif.csv', header=0, sep=',', dtype=str, engine='c')\n",
    "gleif_dict = dict(zip(gleif_df.name, gleif_df.LEI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902df315-a8f2-43e3-9d1c-f1bdddeae1a9",
   "metadata": {},
   "source": [
    "Load the SUB, NUM, and TAG tables into Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d6d11-b078-46ac-9a5b-4c81eca9878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "import uuid\n",
    "\n",
    "# Borrowed/stole this definition from SEC Corp Financials notebook...\n",
    "float_tags = [\n",
    "    'EntityPublicFloat',\n",
    "    'StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest',\n",
    "    'FreeFloat',\n",
    "    'PublicFloat',\n",
    "    'PublicFloatValue',\n",
    "]\n",
    "\n",
    "# These are in priority preference order\n",
    "share_tags = [\n",
    "    'EntityCommonStockSharesOutstanding',\n",
    "    'CommonStockSharesOutstanding',\n",
    "    'SharesOutstanding',\n",
    "    'WeightedAverageNumberOfDilutedSharesOutstanding',\n",
    "    'WeightedAverageNumberOfSharesOutstandingBasic',\n",
    "]\n",
    "\n",
    "# We don't want anything that reports par value, such as CommonStockValue\n",
    "shareprice_tags = [\n",
    "    'SharePrice',\n",
    "    'PerSharePrice',\n",
    "    'MarketValuePerShare',\n",
    "    'SaleOfStockPricePerShare',\n",
    "    'CashPricePerOrdinaryShare',\n",
    "    'TreasurySharesValuePerShare',\n",
    "    'SharesOutstandingPricePerShare',\n",
    "]\n",
    "\n",
    "treasury_share_tags = [\n",
    "    'TreasuryStockShares',\n",
    "    'TreasuryStockShares1',\n",
    "]\n",
    "\n",
    "treasury_value_tags = [\n",
    "    'TreasurySharesMarketValue',\n",
    "    'FairValueOfTreasuryShares',\n",
    "    'MarketValueOfTreasuryShares',\n",
    "]\n",
    "\n",
    "all_float_helper_tags = share_tags + shareprice_tags + treasury_share_tags + treasury_value_tags\n",
    "\n",
    "dera_regex = re.compile(r' ?/.*$')\n",
    "dera_df = {}\n",
    "\n",
    "from math import floor\n",
    "\n",
    "def generate_intermediate_ddate(df_orig):\n",
    "    if len(df_orig)==1:\n",
    "        return df_orig\n",
    "    year1 = df_orig.iloc[0].ddate\n",
    "    year2 = df_orig.iloc[1].ddate\n",
    "    if (year1-year2).days > 731:\n",
    "        print(\"gap years\")\n",
    "        print(df_orig.iloc[0:2])\n",
    "    new_df = df_orig.iloc[[0]].copy()\n",
    "    year_end = pd.to_datetime(f\"{floor((year1.year+year2.year)/2.0)}1231\", format='%Y%m%d', utc=True)\n",
    "    new_df.ddate = year_end\n",
    "    if year1.year==year2.year:\n",
    "        print(\"same years\")\n",
    "        print(df_orig.iloc[0:2])\n",
    "        new_df.value = (df_orig.iloc[0].value + df_orig.iloc[1].value)/2.0\n",
    "    else:\n",
    "        new_df.value = ((365.0-(year1-year_end).days)*df_orig.iloc[0].value + (year_end-year2).days*df_orig.iloc[1].value)/365.0\n",
    "    new_df.version = df_orig.iloc[0].adsh\n",
    "    print(f\"adding fact ({new_df.tag})\")\n",
    "    return new_df\n",
    "\n",
    "# When this function is called, we already know that we have no matches in FLOAT_TAGS.\n",
    "# GROUPED_DF is grouped by ADSH and only for annual reports.  DDATE can be anything (because many reports look back 1-5 years)\n",
    "# We are working these annual reports quarter by quarter for the quarter in which they are reported\n",
    "\n",
    "def infer_float(grouped_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for key, item in grouped_df:\n",
    "        df = grouped_df.get_group(key)\n",
    "        # We have no overall float value.  Build from shares outstanding * price\n",
    "        df_shares = df[df.tag.isin(share_tags) & (df.value>0)]\n",
    "        if df_shares.empty:\n",
    "            continue\n",
    "        else:\n",
    "            for share_tag in share_tags:\n",
    "                if not df_shares[df_shares.tag==share_tag].empty:\n",
    "                    df_shares = df_shares[df_shares.tag==share_tag]\n",
    "                    break\n",
    "            if len(df_shares[df_shares.ddate.dt.year==df_shares.fy.dt.year])>1:\n",
    "                print(\"thinning shares\")\n",
    "                df_shares = df_shares[df_shares.ddate.dt.year==df_shares.fy.dt.year]\n",
    "                df_shares = df_shares.sort_values('ddate', ascending=False)\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        # We have no overall price.  Build from price derived from treasury valuation\n",
    "        if df_prices.empty:\n",
    "            df_treasury_shares = df[df.tag.isin(treasury_share_tags)]\n",
    "            df_treasury_value = df[df.tag.isin(treasury_value_tags)]\n",
    "            if df_treasury_shares.empty or df_treasury_value.empty:\n",
    "                continue\n",
    "            df_svp = df_treasury_value.merge(df_treasury_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                print(f\"{df.adsh.iat[0]}: merge failed (1)\")\n",
    "                continue\n",
    "            # Pick latest date / largest number of shares as basis\n",
    "            df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]].copy()\n",
    "            price_per_share = df_float.value_x.squeeze() / df_float.value_y.squeeze()\n",
    "            df_float.rename(columns={'uom_x':'uom'},inplace=True)\n",
    "            tag = 'ComputedTreasuryFloat'\n",
    "        else:\n",
    "            # if df_prices[df_prices.tag.str.startswith('ShareBasedCompensationArrangementByShareBasedPaymentAward')].empty:\n",
    "            #     print(f\"Must use market prices; len(df_prices) =  {len(df_prices)}\")\n",
    "            # else:\n",
    "            #     print(f\"Can use Share Based Comp {df_prices.tag.str[45:]}:\\n{df_prices}\\n\\n\")\n",
    "            # We derive a price from market reports\n",
    "            df_svp = df_prices.merge(df_shares, on=['adsh', 'ddate', 'coreg'])\n",
    "            if df_svp.empty:\n",
    "                if len (df_prices[df_prices.ddate.dt.year==df_prices.fy.dt.year])>1:\n",
    "                    print(\"thinning prices\")\n",
    "                    df_prices = df_prices[df_prices.ddate.dt.year==df_prices.fy.dt.year]\n",
    "                if len(df_prices)<3 and len(df_shares)<3:\n",
    "                    df_prices = generate_intermediate_ddate(df_prices.sort_values('ddate', ascending=False))\n",
    "                    df_float = generate_intermediate_ddate(df_shares.sort_values('ddate', ascending=False))\n",
    "                    price_per_share = df_prices.value.squeeze()\n",
    "                    # print(\"merge rescued (2)\")\n",
    "                    # display(df_shares)\n",
    "                else:\n",
    "                    print(f\"{df.adsh.iat[0]}: merge failed (2)\")\n",
    "                    print(f\"len(df_prices) = {len(df_prices)}\")\n",
    "                    print(f\"len(df_shares) = {len(df_shares)}\")\n",
    "                    display(df_prices)\n",
    "                    display(df_shares)\n",
    "                    continue\n",
    "            else:\n",
    "                # Pick latest date / largest number of shares as basis\n",
    "                df_float = df_svp.sort_values(['ddate', 'value_y'], ascending=False).iloc[[0]]\n",
    "                price_per_share = df_float.value_x.squeeze() # value_x is a price in this case\n",
    "                df_float.rename(columns={'uom_x':'uom'},inplace=True)\n",
    "            tag = 'ComputedMarketFloat'\n",
    "        df_float = df_float[['adsh', 'ddate', 'uom', 'coreg']]\n",
    "        df_float['tag'] = tag\n",
    "        # TODO: should connect price ddate with total shares ddate\n",
    "        total_shares = df_shares.iloc[0].value\n",
    "        df_float['value'] = price_per_share * total_shares\n",
    "        df_float['qtrs'] = 0\n",
    "        df_float['srcdir'] = 'computed'\n",
    "        df_float['version'] = df_float['footnote'] = pd.NA\n",
    "        df_float = df_float.astype(df.drop(columns=['fy','fp']).dtypes.to_dict())\n",
    "        new_df = new_df.append(df_float)\n",
    "    return new_df\n",
    "\n",
    "def read_dera_table(zf, fy_qtr, tbl):\n",
    "    \"\"\"From a local file ZF, read data for the period FY_QTR for the DERA table TBL.\n",
    "    Return the Dataframe created so that when it is time to create the actual Trino table\n",
    "    we know what the shape of the data should look like.  The returned DF has all the data\n",
    "    of the specific ingestion, not all the data of all the ingestions of data for TBL.\"\"\"\n",
    "    global dera_df\n",
    "    \n",
    "    df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "    df['srcdir'] = fy_qtr\n",
    "    df.srcdir = df.srcdir.astype('string')\n",
    "    \n",
    "    # df = df.convert_dtypes (infer_objects=False, convert_string=True, convert_integer=False, convert_boolean=False, convert_floating=False)\n",
    "    # Print the output\n",
    "    # print(df.dtypes)\n",
    "    \n",
    "    if tbl=='sub':\n",
    "        df.name = df.name.map(lambda x: re.sub(dera_regex, '', x))\n",
    "        df.name = df.name.astype('string')\n",
    "        df['LEI'] = df.name.map(gleif_dict)\n",
    "        df.LEI = df.LEI.astype('string')\n",
    "        df.cik = df.cik.astype('int32')\n",
    "        df.loc[df.sic=='', 'sic'] = pd.NA\n",
    "        df.sic = df.sic.astype('Int16')\n",
    "        df.loc[df.ein=='', 'ein'] = pd.NA\n",
    "        df.ein = df.ein.astype('Int64')\n",
    "        df.wksi = df.wksi.astype('bool')\n",
    "        # df.wksi = df.wksi.astype('int32')\n",
    "        df.period = pd.to_datetime(df.period, format='%Y%m%d', utc=True, errors='coerce')\n",
    "        df.fy = pd.to_datetime(df.fy, format='%Y', utc=True, errors='coerce')\n",
    "        df.filed = pd.to_datetime(df.filed, format='%Y%m%d', utc=True)\n",
    "        df.accepted = pd.to_datetime(df.accepted, format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "        df.prevrpt = df.prevrpt.astype('bool')\n",
    "        df.detail = df.detail.astype('bool')\n",
    "        df.nciks = df.nciks.astype('int16')\n",
    "        \n",
    "        cols = df.columns.tolist()\n",
    "        # Move LEI to a more friendly location in the column order\n",
    "        cols = cols[0:3] + [cols[-1]] + cols[3:-1]\n",
    "        df = df[cols]\n",
    "    elif tbl=='num':\n",
    "        # documentation wrongly lists coreg as NUMERIC length 256.  It is ALPHANUMERIC.\n",
    "        if fy_qtr=='2021q3':\n",
    "            df.loc[df.ddate=='30210630', 'ddate'] = '20210630'\n",
    "        if fy_qtr=='2019q2':\n",
    "            df.loc[df.ddate=='29171231', 'ddate'] = '20171231'\n",
    "        if fy_qtr=='2017q3':\n",
    "            df.loc[df.ddate=='60160630', 'ddate'] = '20160630'\n",
    "        # Fix some bad AES data\n",
    "        if fy_qtr=='2021q1':\n",
    "            df.loc[(df.adsh=='0000874761-21-000015')&(df.tag=='CommonStockValue')&(df.ddate=='20190630'), 'ddate'] = '20200630'\n",
    "        elif fy_qtr=='2020q1':\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='EntityPublicFloat')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "            df.loc[(df.adsh=='0000874761-20-000012')&(df.tag=='CommonStockValue')&(df.ddate=='20180630'), 'ddate'] = '20190630'\n",
    "        df.ddate = pd.to_datetime(df.ddate, format='%Y%m%d', utc=True)\n",
    "        df.qtrs = df.qtrs.astype('int16')\n",
    "        df.loc[df.coreg=='', 'coreg'] = pd.NA\n",
    "        df.loc[df.value=='', 'value'] = pd.NA\n",
    "        df.value = df.value.astype('Float64')\n",
    "        df.loc[df.footnote=='', 'footnote'] = pd.NA\n",
    "        \n",
    "        print(f\"Inferring floats: start len(df) = {len(df)}\")\n",
    "        annual_df = dera_df['sub'][dera_df['sub'].form.isin(['10-K','20-F','40-F'])]\n",
    "        df['fy'] = df.adsh.map(dict(zip(annual_df.adsh,annual_df.fy)))\n",
    "        df['fp'] = df.adsh.map(dict(zip(annual_df.adsh,annual_df.fp)))\n",
    "        print(f\"len(df[df.fp=='FY']) = {len(df[df.fp=='FY'])}\")\n",
    "        df = df[df.fp=='FY']\n",
    "        # df = df.assign(cik=df.adsh.str[:10])\n",
    "        df_has_float = df[df.tag.isin(float_tags)]\n",
    "        print(f\"len(df_has_float) = {len(df_has_float)}\")\n",
    "        df_needs_float = df[~df.adsh.isin(df_has_float.adsh)]\n",
    "        print(f\"len(df_needs_float) = {len(df_needs_float)}\")\n",
    "        float_df = infer_float(df_needs_float[df_needs_float.coreg.isna()\n",
    "                                              &df_needs_float.tag.isin(all_float_helper_tags)\n",
    "                                              &(df_needs_float.value>0)].groupby(['adsh'], as_index=False))\n",
    "        df = df.drop(columns=['fy','fp'])\n",
    "        # print(df.dtypes)\n",
    "        # print(float_df.dtypes)\n",
    "        if float_df.empty:\n",
    "            print(f\"{len(float_df)} floats inferred; Sorry!\")\n",
    "        else:\n",
    "            float_df = float_df.astype(df.dtypes.to_dict())\n",
    "            df = pd.concat([df, float_df])\n",
    "            print(f\"{len(float_df)} floats inferred; {len(float_df[float_df.tag=='ComputedTreasuryFloat'])} treasury-based; {len(float_df[float_df.tag=='ComputedMarketFloat'])} market-based\")\n",
    "    elif tbl=='tag':\n",
    "        df.custom = df.custom.astype('bool')\n",
    "        df.abstract = df.abstract.astype('bool')\n",
    "        df.loc[df.crdr=='', 'crdr'] = pd.NA\n",
    "        df.loc[df.tlabel=='', 'tlabel'] = pd.NA\n",
    "        df.loc[df.doc=='', 'doc'] = pd.NA\n",
    "    # print(df.dtypes)\n",
    "    # display(df.head())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825dd4cb-7510-4b1a-83f5-d1fd1319d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dera_tables = ['tag', 'sub', 'num']\n",
    "\n",
    "if False:\n",
    "    for tbl in dera_tables:\n",
    "        qres = engine.execute(f\"drop table if exists {ingest_schema}.dera_{tbl}\")\n",
    "        print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9abf2-a70b-4341-b14e-22f0f1310340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "objects=source_bucket.objects.filter(Prefix='SEC-DERA/20')\n",
    "\n",
    "# If we are importing from scratch, we may want to consolidate all tags to a single table (ignoring the srcdir field).\n",
    "# But if we are importing incrementally, it's a pain to decide whether a tag has been seen or not, so srcdir keeps each quarterly dataset consistent\n",
    "dera_tag = None\n",
    "batch_size = {'num':1500,'sub':1500,'tag':1000}[tbl]\n",
    "\n",
    "for obj in objects:\n",
    "    if obj.key.endswith('.zip'):\n",
    "        zipfile_src = s3_source.Object(os.environ['S3_LANDING_BUCKET'],obj.key)\n",
    "        tmpname = obj.key.split('/')[-1]\n",
    "        zipfile_src.download_file(f'/tmp/{tmpname}')\n",
    "        zipfile_obj = zipfile.ZipFile(f'/tmp/{tmpname}', mode='r')\n",
    "        fy_qtr = tmpname.split('.')[0]\n",
    "        for tbl in dera_tables:\n",
    "            print(f'{fy_qtr} - {tbl}')\n",
    "            with zipfile_obj.open(f\"{tbl}.txt\") as zf:\n",
    "                # Read data from ZF into a dataframe.\n",
    "                dera_df[tbl] = read_dera_table (zf, fy_qtr, tbl)\n",
    "                if tbl == 'tag':\n",
    "                    dera_tag = pd.concat([dera_tag, dera_df[tbl]]).drop_duplicates(subset=['tag','version'])\n",
    "        zipfile_obj.close()\n",
    "\n",
    "        if False:\n",
    "            # Alas, there is some minor post-fixing we need to do before ingesting into parquet\n",
    "            df = dera_df['num']\n",
    "            num_fields = df.columns\n",
    "            df = df[df.tag=='ComputedTreasuryFloat']\n",
    "            treasury_df = dera_df['sub'].loc[dera_df['sub'].fp=='FY', ['adsh', 'cik', 'name','fye', 'fy', ]].merge(df, on='adsh')\n",
    "            if (len(treasury_df)>0):\n",
    "                display(treasury_df)\n",
    "                grouped_df = treasury_df.groupby('cik')\n",
    "                for key, item in grouped_df:\n",
    "                    if len(item)==1:\n",
    "                        df = item[num_fields].copy()\n",
    "                        df.ddate = pd.to_datetime(f\"{item.fy.squeeze().year}1231\", format='%Y%m%d')\n",
    "                        df.version = item.adsh.squeeze()\n",
    "                        # df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        print(\"adding fact (1)\")\n",
    "                        print(df)\n",
    "                        df = df.astype(dera_df['num'].dtypes.to_dict())\n",
    "                        dera_df['num'] = dera_df['num'].append(df)\n",
    "                    else:\n",
    "                        item = item.sort_values('ddate', ascending=False).reset_index()\n",
    "                        df = generate_intermediate_ddate_value(item.iloc[0:2])\n",
    "                        dera_df['num'] = dera_df['num'].append(df)\n",
    "        if False:\n",
    "            # Now output as parquet files\n",
    "            for tbl in dera_tables:\n",
    "                buf = io.BytesIO()\n",
    "                dera_df[tbl].to_parquet(path=buf)\n",
    "                buf.seek(0)\n",
    "                fy_qtr = dera_df[tbl].iloc[0].srcdir\n",
    "                trino_bucket.upload_fileobj(Fileobj=buf,\n",
    "                                            Key=f'trino/{ingest_schema}/{tbl}/{fy_qtr}.parquet')\n",
    "        else:\n",
    "            # Import into Trino the slow way...but not tag\n",
    "            for tbl in dera_tables:\n",
    "                if tbl == 'tag':\n",
    "                    continue\n",
    "                ingest_table = f\"dera_{tbl}\"\n",
    "                columnschema = osc.create_table_schema_pairs(dera_df[tbl],\n",
    "                                                             typemap={\"int16\":\"integer\", \"Int16\":\"integer\",\n",
    "                                                                      \"datetime64[ns, UTC]\":\"timestamp(6)\"})\n",
    "                tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{ingest_table}(\n",
    "{columnschema}\n",
    ") with (\n",
    "    partitioning = array['srcdir'],\n",
    "    format = 'ORC'\n",
    ")\n",
    "\"\"\"\n",
    "                print(tabledef)\n",
    "                qres = engine.execute(tabledef)\n",
    "                print(qres.fetchall())\n",
    "                dera_df[tbl].to_sql(ingest_table,\n",
    "                                    con=engine, schema=ingest_schema, if_exists='append',\n",
    "                                    index=False,\n",
    "                                    method=osc.TrinoBatchInsert(batch_size=batch_size[tbl], verbose = False))\n",
    "\n",
    "ingest_table = f\"dera_tag\"\n",
    "columnschema = osc.create_table_schema_pairs(dera_tag,\n",
    "                                             typemap={\"int16\":\"integer\", \"Int16\":\"integer\",\n",
    "                                                      \"datetime64[ns, UTC]\":\"timestamp(6)\"})\n",
    "tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{ingest_table}(\n",
    "{columnschema}\n",
    ") with (\n",
    "partitioning = array['bucket(version,10)'],\n",
    "format = 'ORC'\n",
    ")\n",
    "\"\"\"\n",
    "print(tabledef)\n",
    "qres = engine.execute(tabledef)\n",
    "print(qres.fetchall())\n",
    "dera_tag.to_sql('dera_tag',\n",
    "                con=engine, schema=ingest_schema, if_exists='append',\n",
    "                index=False,\n",
    "                method=osc.TrinoBatchInsert(batch_size=batch_size['tag'], verbose = False))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35a95b38-b5a7-4b6f-a454-01ac62e622a4",
   "metadata": {},
   "source": [
    "# Once we have all our parquet files in place, load up the tables with their directory contents\n",
    "for tbl in dera_tables:\n",
    "    if tbl not in dera_df:\n",
    "        error(f'{tbl} data not found')\n",
    "    tabledef = osc.unmanaged_parquet_tabledef(dera_df[tbl],\n",
    "                                              ingest_catalog, ingest_schema, tbl, trino_bucket,\n",
    "                                              typemap={'int16':'smallint', 'Int16':'smallint'})\n",
    "    qres = engine.execute(tabledef)\n",
    "    for row in qres.fetchall():\n",
    "        print(row)\n",
    "\n",
    "    dataset_query = (f'SELECT * FROM {ingest_catalog}.{ingest_schema}.{tbl} limit 10')\n",
    "    print(dataset_query)\n",
    "    dataset = engine.execute(dataset_query)\n",
    "    for row in dataset.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "772604b8-7ba1-475b-8257-831c523c65e0",
   "metadata": {},
   "source": [
    "zipfile_obj = zipfile.ZipFile(f'/tmp/2021q2.zip', mode='r')\n",
    "fy_qtr = tmpname.split('.')[0]\n",
    "for zipinfo in zipfile_obj.infolist():\n",
    "    fname = zipinfo.filename\n",
    "    if fname != 'num.txt':\n",
    "        continue\n",
    "    if fname[3:] != '.txt':\n",
    "        continue\n",
    "    tbl = fname[:3]\n",
    "    if tbl not in dera_tables:\n",
    "        continue\n",
    "    ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "    print(f'{fy_qtr} - {tbl}')\n",
    "    with zipfile_obj.open(fname) as zf:\n",
    "        # This fills a directory with parquet files\n",
    "        df = pd.read_csv(zf, header=0, sep='\\t', dtype='string', keep_default_na=False, nrows = None, engine='c')\n",
    "        df_shares = df[df.tag.isin(share_tags)]\n",
    "        df_prices = df[df.tag.isin(shareprice_tags)]\n",
    "        print(len(df_shares))\n",
    "        print(len(df_prices))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "714094ff-06b1-4192-8a2e-664c354d70a5",
   "metadata": {},
   "source": [
    "df.loc[df.value=='', 'value'] = pd.NA\n",
    "df[(df.adsh=='0001193125-21-195161')&(df.tag=='SharesOutstanding')].sort_values('value',ascending=False).iloc[0]['value'] is pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375a547-4e34-484b-8ff4-b40416be315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrow metadata code from DERA-iceberg if/when we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285fe0e-e7ce-4c5b-8f2d-d6496e766ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablenames = ['dera_sub', 'dera_num', 'dera_tag']\n",
    "qres = engine.execute(f'select count(*) from {ingest_catalog}.{ingest_schema}.ticker').fetchall()\n",
    "print(\"ticker count(*) = \" + str(qres[0]))\n",
    "for tbl in tablenames:\n",
    "    qres = engine.execute(f'select count (*),srcdir from {ingest_catalog}.{ingest_schema}.{tbl} group by srcdir order by srcdir').fetchall()\n",
    "    for row in qres:\n",
    "        print(f'{tbl}[{row[1]}] count(*) = {row[0]}')\n",
    "    print(\"\")\n",
    "# print(list(zip(tablenames, l)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
